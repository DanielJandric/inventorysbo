import requests
from bs4 import BeautifulSoup
import time
import random
import logging
from typing import Dict, List, Optional
from datetime import datetime

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ReutersScraper:
    """
    Scraper pour r√©cup√©rer les donn√©es financi√®res depuis Reuters.com et NZZ.ch
    """
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        # URLs Reuters pour diff√©rents march√©s
        self.urls = {
            'indices': {
                'S&P 500': 'https://www.reuters.com/markets/indices',
                'NASDAQ': 'https://www.reuters.com/markets/indices',
                'Dow Jones': 'https://www.reuters.com/markets/indices',
                'CAC 40': 'https://www.reuters.com/markets/indices',
                'DAX': 'https://www.reuters.com/markets/indices',
                'SMI': 'https://www.reuters.com/markets/indices'
            },
            'crypto': {
                'Bitcoin': 'https://www.reuters.com/markets/currencies',
                'Ethereum': 'https://www.reuters.com/markets/currencies'
            },
            'bonds': {
                'US 10Y': 'https://www.reuters.com/markets/rates-bonds',
                'Bund 10Y': 'https://www.reuters.com/markets/rates-bonds'
            }
        }
        
        # URLs NZZ pour les actualit√©s suisses
        self.nzz_urls = [
            'https://www.nzz.ch/wirtschaft',      # √âconomie
            'https://www.nzz.ch/finanzen',        # Finances
            'https://www.nzz.ch/unternehmen',     # Entreprises
            'https://www.nzz.ch/international',   # International
            'https://www.nzz.ch/schweiz',         # Suisse
            'https://www.nzz.ch/meinung'          # Opinions/√âditoriaux
        ]
        
        # URLs AP News pour les actualit√©s internationales
        self.ap_urls = [
            'https://apnews.com/business',        # Business
            'https://apnews.com/politics',        # Politique
            'https://apnews.com/technology',      # Technologie
            'https://apnews.com/science',         # Science
            'https://apnews.com/health'           # Sant√©
        ]
    
    def get_market_data(self) -> Dict[str, str]:
        """
        R√©cup√®re toutes les donn√©es de march√© depuis Reuters
        """
        logger.info("üîç D√©but du scraping Reuters...")
        
        market_data = {}
        
        try:
            # Scraper les indices
            indices_data = self._scrape_indices()
            market_data.update(indices_data)
            
            # Pause pour √©viter d'√™tre d√©tect√©
            time.sleep(random.uniform(1, 3))
            
            # Scraper les crypto
            crypto_data = self._scrape_crypto()
            market_data.update(crypto_data)
            
            # Pause
            time.sleep(random.uniform(1, 3))
            
            # Scraper les obligations
            bonds_data = self._scrape_bonds()
            market_data.update(bonds_data)
            
            # Si aucune donn√©e r√©cup√©r√©e, essayer des sources alternatives
            if not market_data:
                logger.warning("‚ö†Ô∏è Aucune donn√©e de march√© r√©cup√©r√©e, essai sources alternatives...")
                alternative_data = self._get_alternative_market_data()
                market_data.update(alternative_data)
            
            logger.info(f"‚úÖ Scraping termin√© - {len(market_data)} donn√©es r√©cup√©r√©es")
            return market_data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du scraping: {e}")
            return {}
    
    def _scrape_indices(self) -> Dict[str, str]:
        """
        Scrape les indices boursiers
        """
        logger.info("üìä Scraping des indices...")
        
        try:
            url = "https://www.reuters.com/markets/indices"
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Chercher les donn√©es des indices dans la page
            indices_data = {}
            
            # Recherche des √©l√©ments contenant les donn√©es d'indices
            # (√† adapter selon la structure HTML de Reuters)
            index_elements = soup.find_all(['div', 'span'], class_=lambda x: x and any(keyword in x.lower() for keyword in ['index', 'market', 'price', 'quote']))
            
            for element in index_elements:
                text = element.get_text().strip()
                if any(index in text for index in ['S&P', 'NASDAQ', 'Dow', 'CAC', 'DAX', 'SMI']):
                    logger.info(f"Trouv√©: {text}")
                    # Extraire le nom et la valeur
                    # (logique √† affiner selon le format exact)
            
            return indices_data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur scraping indices: {e}")
            return {}
    
    def _scrape_crypto(self) -> Dict[str, str]:
        """
        Scrape les crypto-monnaies
        """
        logger.info("ü™ô Scraping des crypto...")
        
        try:
            url = "https://www.reuters.com/markets/currencies"
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Logique similaire pour les crypto
            crypto_data = {}
            
            return crypto_data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur scraping crypto: {e}")
            return {}
    
    def _scrape_bonds(self) -> Dict[str, str]:
        """
        Scrape les obligations
        """
        logger.info("üìà Scraping des obligations...")
        
        try:
            url = "https://www.reuters.com/markets/rates-bonds"
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Logique similaire pour les obligations
            bonds_data = {}
            
            return bonds_data
            
        except Exception as e:
            logger.error(f"‚ùå Erreur scraping obligations: {e}")
            return {}
    
    def get_financial_news(self, max_news: int = 5) -> List[Dict[str, str]]:
        """
        Scrape les actualit√©s financi√®res depuis Reuters, NZZ et AP News
        """
        logger.info("üì∞ Scraping des actualit√©s financi√®res...")
        
        news_list = []
        
        # Scraper Reuters
        reuters_news = self._scrape_reuters_news(max_news // 3)
        news_list.extend(reuters_news)
        
        # Scraper NZZ
        nzz_news = self._scrape_nzz_news(max_news // 3)
        news_list.extend(nzz_news)
        
        # Scraper AP News
        ap_news = self._scrape_ap_news(max_news // 3)
        news_list.extend(ap_news)
        
        # Si aucune actualit√© r√©cup√©r√©e, essayer des sources alternatives
        if not news_list:
            logger.warning("‚ö†Ô∏è Aucune actualit√© r√©cup√©r√©e, essai sources alternatives...")
            alternative_news = self._scrape_alternative_sources(max_news)
            news_list.extend(alternative_news)
        
        logger.info(f"‚úÖ {len(news_list)} actualit√©s r√©cup√©r√©es (Reuters: {len(reuters_news)}, NZZ: {len(nzz_news)}, AP: {len(ap_news)})")
        return news_list[:max_news]
    
    def _scrape_reuters_news(self, max_news: int) -> List[Dict[str, str]]:
        """
        Scrape les actualit√©s depuis Reuters
        """
        logger.info("üì∞ Scraping Reuters...")
        
        news_list = []
        
        try:
            # URLs des sections d'actualit√©s √©largies
            news_urls = [
                "https://www.reuters.com/markets/finance/",
                "https://www.reuters.com/markets/",
                "https://www.reuters.com/business/",
                "https://www.reuters.com/technology/",
                "https://www.reuters.com/politics/",
                "https://www.reuters.com/world/",
                "https://www.reuters.com/companies/",
                "https://www.reuters.com/economy/"
            ]
            
            for url in news_urls:
                try:
                    response = self.session.get(url, timeout=10)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Chercher les articles d'actualit√©s
                    articles = soup.find_all(['article', 'div'], class_=lambda x: x and any(keyword in x.lower() for keyword in ['article', 'story', 'news', 'headline']))
                    
                    for article in articles[:max_news//len(news_urls)]:  # R√©partir les articles entre les sections
                        # Extraire le titre
                        title_element = article.find(['h1', 'h2', 'h3', 'h4', 'a'])
                        if title_element:
                            title = title_element.get_text().strip()
                            
                            # Extraire le r√©sum√©/description
                            summary_element = article.find(['p', 'div'], class_=lambda x: x and any(keyword in x.lower() for keyword in ['summary', 'description', 'excerpt']))
                            summary = summary_element.get_text().strip() if summary_element else ""
                            
                            # Extraire l'URL
                            link_element = article.find('a')
                            url = link_element.get('href') if link_element else ""
                            if url and not url.startswith('http'):
                                url = f"https://www.reuters.com{url}"
                            
                            if title and len(title) > 10:  # Filtrer les titres trop courts
                                news_list.append({
                                    'title': title,
                                    'summary': summary,
                                    'url': url,
                                    'source': 'Reuters'
                                })
                    
                    # Pause entre les requ√™tes
                    time.sleep(random.uniform(1, 2))
                    
                except Exception as e:
                    logger.error(f"‚ùå Erreur scraping news depuis {url}: {e}")
                    continue
            
            logger.info(f"‚úÖ {len(news_list)} actualit√©s r√©cup√©r√©es")
            return news_list[:max_news]  # Garder le max demand√©
            
        except Exception as e:
            logger.error(f"‚ùå Erreur scraping Reuters: {e}")
            return []
    
    def _scrape_nzz_news(self, max_news: int) -> List[Dict[str, str]]:
        """
        Scrape les actualit√©s depuis NZZ.ch
        """
        logger.info("üì∞ Scraping NZZ...")
        
        news_list = []
        
        try:
            for url in self.nzz_urls:
                try:
                    response = self.session.get(url, timeout=10)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Chercher les articles NZZ (structure sp√©cifique)
                    articles = soup.find_all(['article', 'div'], class_=lambda x: x and any(keyword in x.lower() for keyword in ['article', 'story', 'news', 'headline', 'teaser']))
                    
                    for article in articles[:max_news//len(self.nzz_urls)]:
                        # Extraire le titre
                        title_element = article.find(['h1', 'h2', 'h3', 'h4', 'a', 'h5'])
                        if title_element:
                            title = title_element.get_text().strip()
                            
                            # Extraire le r√©sum√©/description
                            summary_element = article.find(['p', 'div'], class_=lambda x: x and any(keyword in x.lower() for keyword in ['summary', 'description', 'excerpt', 'teaser']))
                            summary = summary_element.get_text().strip() if summary_element else ""
                            
                            # Extraire l'URL
                            link_element = article.find('a')
                            article_url = link_element.get('href') if link_element else ""
                            if article_url and not article_url.startswith('http'):
                                article_url = f"https://www.nzz.ch{article_url}"
                            
                            if title and len(title) > 10:  # Filtrer les titres trop courts
                                news_list.append({
                                    'title': title,
                                    'summary': summary,
                                    'url': article_url,
                                    'source': 'NZZ'
                                })
                    
                    # Pause entre les requ√™tes
                    time.sleep(random.uniform(1, 2))
                    
                except Exception as e:
                    logger.error(f"‚ùå Erreur scraping NZZ depuis {url}: {e}")
                    continue
            
            return news_list[:max_news]
            
        except Exception as e:
            logger.error(f"‚ùå Erreur scraping NZZ: {e}")
            return []
    
    def _scrape_ap_news(self, max_news: int) -> List[Dict[str, str]]:
        """
        Scrape les actualit√©s depuis AP News
        """
        logger.info("üì∞ Scraping AP News...")
        
        news_list = []
        
        try:
            for url in self.ap_urls:
                try:
                    response = self.session.get(url, timeout=10)
                    response.raise_for_status()
                    
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Chercher les articles AP News (structure sp√©cifique)
                    articles = soup.find_all(['article', 'div'], class_=lambda x: x and any(keyword in x.lower() for keyword in ['article', 'story', 'news', 'headline', 'card', 'teaser']))
                    
                    for article in articles[:max_news//len(self.ap_urls)]:
                        # Extraire le titre
                        title_element = article.find(['h1', 'h2', 'h3', 'h4', 'a', 'h5'])
                        if title_element:
                            title = title_element.get_text().strip()
                            
                            # Extraire le r√©sum√©/description
                            summary_element = article.find(['p', 'div'], class_=lambda x: x and any(keyword in x.lower() for keyword in ['summary', 'description', 'excerpt', 'teaser', 'lede']))
                            summary = summary_element.get_text().strip() if summary_element else ""
                            
                            # Extraire l'URL
                            link_element = article.find('a')
                            article_url = link_element.get('href') if link_element else ""
                            if article_url and not article_url.startswith('http'):
                                article_url = f"https://apnews.com{article_url}"
                            
                            if title and len(title) > 10:  # Filtrer les titres trop courts
                                news_list.append({
                                    'title': title,
                                    'summary': summary,
                                    'url': article_url,
                                    'source': 'AP News'
                                })
                    
                    # Pause entre les requ√™tes
                    time.sleep(random.uniform(1, 2))
                    
                except Exception as e:
                    logger.error(f"‚ùå Erreur scraping AP News depuis {url}: {e}")
                    continue
            
            return news_list[:max_news]
            
        except Exception as e:
            logger.error(f"‚ùå Erreur scraping AP News: {e}")
            return []
    
    def _scrape_alternative_sources(self, max_news: int) -> List[Dict[str, str]]:
        """
        Scrape des sources alternatives si les principales √©chouent
        """
        logger.info("üì∞ Scraping sources alternatives...")
        
        news_list = []
        
        # Sources alternatives plus simples
        alternative_urls = [
            'https://www.bloomberg.com/markets',
            'https://www.ft.com/markets',
            'https://www.cnbc.com/markets/'
        ]
        
        for url in alternative_urls:
            try:
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # Chercher les titres d'articles
                articles = soup.find_all(['h1', 'h2', 'h3', 'h4'], class_=lambda x: x and any(keyword in x.lower() for keyword in ['headline', 'title', 'story']))
                
                for article in articles[:max_news//len(alternative_urls)]:
                    title = article.get_text().strip()
                    
                    if title and len(title) > 10:
                        news_list.append({
                            'title': title,
                            'summary': "",
                            'url': url,
                            'source': 'Alternative'
                        })
                
                time.sleep(random.uniform(1, 2))
                
            except Exception as e:
                logger.error(f"‚ùå Erreur scraping alternative {url}: {e}")
                continue
        
        return news_list[:max_news]
    
    def _get_alternative_market_data(self) -> Dict[str, str]:
        """
        R√©cup√®re des donn√©es de march√© depuis des sources alternatives
        """
        logger.info("üìä R√©cup√©ration donn√©es alternatives...")
        
        market_data = {}
        
        try:
            # Utiliser Yahoo Finance comme fallback
            import yfinance as yf
            
            symbols = {
                'S&P 500': '^GSPC',
                'NASDAQ': '^IXIC', 
                'Dow Jones': '^DJI',
                'Bitcoin': 'BTC-USD',
                'Ethereum': 'ETH-USD'
            }
            
            for name, symbol in symbols.items():
                try:
                    ticker = yf.Ticker(symbol)
                    info = ticker.info
                    
                    if 'currentPrice' in info and info['currentPrice']:
                        price = info['currentPrice']
                        change = info.get('regularMarketChange', 0)
                        change_percent = info.get('regularMarketChangePercent', 0)
                        market_data[name] = f"${price:.2f} ({change:+.2f}, {change_percent:+.2f}%)"
                    
                except Exception as e:
                    logger.error(f"‚ùå Erreur Yahoo Finance {name}: {e}")
                    continue
            
            return market_data
            
        except ImportError:
            logger.error("‚ùå Yahoo Finance non disponible")
            return {}
        except Exception as e:
            logger.error(f"‚ùå Erreur donn√©es alternatives: {e}")
            return {}
    
    def format_market_data(self, data: Dict[str, str]) -> str:
        """
        Formate les donn√©es pour le prompt Gemini
        """
        if not data:
            return "Aucune donn√©e disponible via Reuters"
        
        formatted_lines = []
        for name, value in data.items():
            formatted_lines.append(f"{name}: {value}")
        
        return "\n".join(formatted_lines)
    
    def format_news_data(self, news_list: List[Dict[str, str]]) -> str:
        """
        Formate les actualit√©s pour le prompt Gemini avec liens
        """
        if not news_list:
            return "Aucune actualit√© disponible"
        
        formatted_lines = ["ACTUALIT√âS R√âCENTES (Reuters + NZZ + AP News):"]
        
        # Cat√©goriser les actualit√©s par mots-cl√©s
        categories = {
            'FINANCE': [],
            'TECHNOLOGIE': [],
            'POLITIQUE': [],
            '√âCONOMIE': [],
            'MONDE': [],
            'ENTREPRISES': []
        }
        
        for news in news_list:
            title_lower = news['title'].lower()
            
            if any(word in title_lower for word in ['bank', 'finance', 'market', 'trading', 'stock', 'bond', 'crypto']):
                categories['FINANCE'].append(news)
            elif any(word in title_lower for word in ['tech', 'ai', 'artificial intelligence', 'software', 'digital', 'cyber']):
                categories['TECHNOLOGIE'].append(news)
            elif any(word in title_lower for word in ['politic', 'election', 'government', 'congress', 'parliament']):
                categories['POLITIQUE'].append(news)
            elif any(word in title_lower for word in ['economy', 'gdp', 'inflation', 'interest rate', 'fed', 'ecb']):
                categories['√âCONOMIE'].append(news)
            elif any(word in title_lower for word in ['china', 'russia', 'ukraine', 'europe', 'asia', 'middle east']):
                categories['MONDE'].append(news)
            else:
                categories['ENTREPRISES'].append(news)
        
        # Formater par cat√©gorie avec liens
        for category, articles in categories.items():
            if articles:
                formatted_lines.append(f"\nüì∞ {category}:")
                for i, news in enumerate(articles, 1):
                    source_tag = f"[{news['source']}]" if 'source' in news else ""
                    url_tag = f"({news['url']})" if news.get('url') else ""
                    formatted_lines.append(f"   {i}. {news['title']} {source_tag}")
                    if news['summary']:
                        formatted_lines.append(f"      {news['summary'][:100]}...")
                    if url_tag:
                        formatted_lines.append(f"      Lien: {url_tag}")
        
        return "\n".join(formatted_lines)

def test_reuters_scraper():
    """
    Test du scraper Reuters
    """
    scraper = ReutersScraper()
    
    print("üß™ Test du scraper Reuters...")
    print("=" * 50)
    
    # Test de connexion
    try:
        response = scraper.session.get("https://www.reuters.com", timeout=5)
        print(f"‚úÖ Connexion Reuters: {response.status_code}")
    except Exception as e:
        print(f"‚ùå Erreur connexion: {e}")
        return
    
    # Test du scraping des donn√©es de march√©
    print("\nüìä Test des donn√©es de march√©...")
    market_data = scraper.get_market_data()
    
    if market_data:
        for name, value in market_data.items():
            print(f"{name}: {value}")
    else:
        print("Aucune donn√©e de march√© r√©cup√©r√©e")
    
    # Test du scraping des actualit√©s
    print("\nüì∞ Test des actualit√©s financi√®res...")
    news_data = scraper.get_financial_news(max_news=3)
    
    if news_data:
        for i, news in enumerate(news_data, 1):
            print(f"{i}. {news['title']}")
            if news['summary']:
                print(f"   {news['summary']}")
            print()
    else:
        print("Aucune actualit√© r√©cup√©r√©e")
    
    # Format pour Gemini
    print("\nüìù Format complet pour Gemini:")
    print("=" * 40)
    
    market_formatted = scraper.format_market_data(market_data)
    news_formatted = scraper.format_news_data(news_data)
    
    print("DONN√âES DE MARCH√â:")
    print(market_formatted)
    print("\n" + "="*40 + "\n")
    print(news_formatted)

if __name__ == "__main__":
    test_reuters_scraper() 