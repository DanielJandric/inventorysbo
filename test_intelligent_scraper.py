#!/usr/bin/env python3
"""
Test du scraper intelligent
"""
import asyncio
import json
from intelligent_scraper import IntelligentScraper

async def test_scraper():
    """Test complet du scraper intelligent"""
    print("ğŸ§ª Test du Scraper Intelligent")
    print("=" * 50)
    
    # Initialiser le scraper
    scraper = IntelligentScraper()
    await scraper.initialize()
    
    try:
        # Test 1: CrÃ©er une tÃ¢che
        print("\nğŸ“‹ Test 1: CrÃ©ation de tÃ¢che")
        task_id = await scraper.create_scraping_task(
            "Tesla stock price today latest news earnings",
            num_results=3
        )
        print(f"âœ… TÃ¢che crÃ©Ã©e: {task_id}")
        
        # Test 2: ExÃ©cuter la tÃ¢che
        print("\nğŸš€ Test 2: ExÃ©cution de la tÃ¢che")
        result = await scraper.execute_scraping_task(task_id)
        
        if "error" in result:
            print(f"âŒ Erreur: {result['error']}")
            return
        
        print("âœ… TÃ¢che exÃ©cutÃ©e avec succÃ¨s")
        print(f"ğŸ“Š RÃ©sultats:")
        print(f"   ğŸ“ RÃ©sumÃ©: {result.get('summary', 'N/A')[:200]}...")
        print(f"   ğŸ“ˆ Points clÃ©s: {len(result.get('key_points', []))}")
        print(f"   ğŸ’¡ Insights: {len(result.get('insights', []))}")
        print(f"   ğŸ¯ Recommandations: {len(result.get('recommendations', []))}")
        print(f"   ğŸ“š Sources: {len(result.get('sources', []))}")
        print(f"   ğŸ¯ Score de confiance: {result.get('confidence_score', 0)}")
        
        # Test 3: VÃ©rifier le statut
        print("\nğŸ“Š Test 3: VÃ©rification du statut")
        task = await scraper.get_task_status(task_id)
        if task:
            print(f"âœ… Statut: {task.status}")
            print(f"   CrÃ©Ã©: {task.created_at}")
            print(f"   TerminÃ©: {task.completed_at}")
        else:
            print("âŒ TÃ¢che non trouvÃ©e")
        
        # Test 4: Test direct de scraping
        print("\nğŸ” Test 4: Scraping direct")
        scraped_data = await scraper.search_and_scrape("Apple stock price today", num_results=2)
        print(f"âœ… {len(scraped_data)} pages scrapÃ©es")
        
        for i, data in enumerate(scraped_data):
            print(f"   ğŸ“„ {i+1}. {data.title}")
            print(f"      URL: {data.url}")
            print(f"      Contenu: {len(data.content)} caractÃ¨res")
        
        # Test 5: Traitement LLM
        print("\nğŸ¤– Test 5: Traitement LLM")
        if scraped_data:
            llm_result = await scraper.process_with_llm(
                "Analysez les informations sur Apple et donnez-moi un rÃ©sumÃ©",
                scraped_data
            )
            print(f"âœ… Traitement LLM terminÃ©")
            print(f"   ğŸ“ RÃ©sumÃ©: {llm_result.get('summary', 'N/A')[:150]}...")
            print(f"   ğŸ“ˆ Points clÃ©s: {len(llm_result.get('key_points', []))}")
        
    except Exception as e:
        print(f"âŒ Erreur lors du test: {e}")
    
    finally:
        await scraper.cleanup()
        print("\nğŸ§¹ Scraper nettoyÃ©")

def test_sync():
    """Test synchrone pour vÃ©rifier l'import"""
    print("ğŸ”§ Test d'import synchrone")
    try:
        from intelligent_scraper import IntelligentScraper
        scraper = IntelligentScraper()
        print("âœ… Import rÃ©ussi")
        return True
    except Exception as e:
        print(f"âŒ Erreur d'import: {e}")
        return False

if __name__ == "__main__":
    print("ğŸš€ DÃ©marrage des tests du scraper intelligent")
    
    # Test d'import
    if test_sync():
        # Test asynchrone
        asyncio.run(test_scraper())
    else:
        print("âŒ Impossible de continuer les tests") 