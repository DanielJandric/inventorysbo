#!/usr/bin/env python3
"""
Scraper intelligent pour collecter et analyser des donnÃ©es web
"""
import os
import asyncio
import json
import uuid
from datetime import datetime
from typing import List, Dict, Optional
from dataclasses import dataclass
from dotenv import load_dotenv
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from openai import OpenAI
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ScrapedData:
    """DonnÃ©es scrapÃ©es d'une page web"""
    url: str
    title: str
    content: str
    timestamp: datetime
    metadata: Dict

@dataclass
class ScrapingTask:
    """TÃ¢che de scraping"""
    id: str
    prompt: str
    status: str  # pending, processing, completed, failed
    results: Optional[Dict] = None
    created_at: datetime = None
    completed_at: Optional[datetime] = None
    error: Optional[str] = None

class IntelligentScraper:
    """Scraper intelligent utilisant Playwright et OpenAI"""
    
    def __init__(self):
        load_dotenv()
        self.playwright = None
        self.browser = None
        self.openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        self.tasks = {}  # Stockage temporaire des tÃ¢ches
        self._initialized = False
        
    def initialize_sync(self):
        """Initialise le navigateur Playwright de maniÃ¨re synchrone"""
        try:
            if not self._initialized:
                # Utiliser asyncio.run pour initialiser de maniÃ¨re synchrone
                asyncio.run(self._async_initialize())
                self._initialized = True
                logger.info("âœ… Scraper intelligent initialisÃ© (synchrone)")
        except Exception as e:
            logger.error(f"âŒ Erreur initialisation scraper synchrone: {e}")
            raise
    
    async def _async_initialize(self):
        """Initialisation asynchrone interne"""
        try:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-dev-shm-usage', '--disable-gpu']
            )
            logger.info("âœ… Navigateur Playwright initialisÃ©")
        except Exception as e:
            logger.error(f"âŒ Erreur initialisation navigateur: {e}")
            raise
    
    async def initialize(self):
        """Initialise le navigateur Playwright (mÃ©thode asynchrone)"""
        if not self._initialized:
            await self._async_initialize()
            self._initialized = True
            logger.info("âœ… Scraper intelligent initialisÃ©")
    
    async def create_scraping_task(self, prompt: str, num_results: int = 5) -> str:
        """CrÃ©e une nouvelle tÃ¢che de scraping"""
        task_id = str(uuid.uuid4())
        task = ScrapingTask(
            id=task_id,
            prompt=prompt,
            status="pending",
            created_at=datetime.now()
        )
        self.tasks[task_id] = task
        logger.info(f"ğŸ“‹ TÃ¢che de scraping crÃ©Ã©e: {task_id}")
        return task_id
    
    async def search_and_scrape(self, query: str, num_results: int = 5) -> List[ScrapedData]:
        """Recherche sur Google et scrape les rÃ©sultats pertinents"""
        # S'assurer que le navigateur est initialisÃ©
        if not self._initialized or not self.browser:
            await self.initialize()
            
        page = await self.browser.new_page()
        results = []
        
        try:
            # Recherche Google
            search_url = f"https://www.google.com/search?q={query}"
            logger.info(f"ğŸ” Recherche: {query}")
            
            await page.goto(search_url, wait_until='domcontentloaded')
            
            # Attendre que la page soit chargÃ©e avec des sÃ©lecteurs alternatifs
            selectors_to_try = [
                'h3',
                'div[data-sokoban-container] h3',
                'div[jscontroller] h3',
                'a[href] h3',
                'div[role="heading"]',
                'h1, h2, h3'
            ]
            
            page_loaded = False
            for selector in selectors_to_try:
                try:
                    await page.wait_for_selector(selector, timeout=5000)
                    page_loaded = True
                    logger.info(f"âœ… SÃ©lecteur trouvÃ©: {selector}")
                    break
                except Exception as e:
                    logger.warning(f"âš ï¸ SÃ©lecteur {selector} non trouvÃ©: {e}")
                    continue
            
            if not page_loaded:
                logger.warning("âš ï¸ Aucun sÃ©lecteur trouvÃ©, tentative de scraping direct")
                # Attendre un peu et essayer de continuer
                await page.wait_for_timeout(3000)
            
            # Extraction des liens avec plusieurs mÃ©thodes
            links = []
            
            # MÃ©thode 1: SÃ©lecteur h3 dans les liens
            try:
                links = await page.evaluate("""
                    () => {
                        const links = [];
                        document.querySelectorAll('a h3').forEach((h3) => {
                            const link = h3.closest('a');
                            if (link && link.href && !link.href.includes('google.com')) {
                                links.push({
                                    url: link.href,
                                    title: h3.innerText
                                });
                            }
                        });
                        return links.slice(0, 5);
                    }
                """)
            except Exception as e:
                logger.warning(f"âš ï¸ MÃ©thode 1 Ã©chouÃ©e: {e}")
            
            # MÃ©thode 2: Recherche plus large si la premiÃ¨re Ã©choue
            if not links:
                try:
                    links = await page.evaluate("""
                        () => {
                            const links = [];
                            document.querySelectorAll('a[href]').forEach((link) => {
                                if (link.href && 
                                    !link.href.includes('google.com') && 
                                    !link.href.includes('youtube.com') &&
                                    link.innerText.trim().length > 10) {
                                    const title = link.querySelector('h3, h2, div')?.innerText || link.innerText;
                                    if (title && title.trim().length > 5) {
                                        links.push({
                                            url: link.href,
                                            title: title.trim()
                                        });
                                    }
                                }
                            });
                            return links.slice(0, 5);
                        }
                    """)
                except Exception as e:
                    logger.warning(f"âš ï¸ MÃ©thode 2 Ã©chouÃ©e: {e}")
            
            # MÃ©thode 3: Recherche par texte si les autres Ã©chouent
            if not links:
                try:
                    links = await page.evaluate("""
                        () => {
                            const links = [];
                            const allLinks = Array.from(document.querySelectorAll('a[href]'));
                            const validLinks = allLinks.filter(link => 
                                link.href && 
                                !link.href.includes('google.com') && 
                                !link.href.includes('youtube.com') &&
                                link.innerText.trim().length > 10
                            );
                            
                            validLinks.slice(0, 5).forEach(link => {
                                links.push({
                                    url: link.href,
                                    title: link.innerText.trim().substring(0, 100)
                                });
                            });
                            return links;
                        }
                    """)
                except Exception as e:
                    logger.warning(f"âš ï¸ MÃ©thode 3 Ã©chouÃ©e: {e}")
            
            logger.info(f"ğŸ“„ {len(links)} liens trouvÃ©s")
            
            # Si aucun lien trouvÃ©, crÃ©er un rÃ©sultat factice pour Ã©viter l'erreur
            if not links:
                logger.warning("âš ï¸ Aucun lien trouvÃ©, crÃ©ation d'un rÃ©sultat factice")
                results.append(ScrapedData(
                    url="https://example.com",
                    title="Aucun rÃ©sultat trouvÃ©",
                    content=f"Recherche pour '{query}' - Aucun rÃ©sultat disponible. Cela peut Ãªtre dÃ» Ã  des restrictions de Google ou Ã  un problÃ¨me de rÃ©seau.",
                    timestamp=datetime.now(),
                    metadata={
                        'word_count': 20,
                        'language': 'fr',
                        'note': 'RÃ©sultat factice - aucun lien trouvÃ©'
                    }
                ))
                return results
            
            # Scraping de chaque page
            for i, link in enumerate(links[:num_results]):
                try:
                    logger.info(f"ğŸ“– Scraping {i+1}/{len(links)}: {link['title']}")
                    await page.goto(link['url'], timeout=15000, wait_until='domcontentloaded')
                    content = await page.content()
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # Extraction intelligente du contenu
                    text_content = self._extract_main_content(soup)
                    
                    scraped_data = ScrapedData(
                        url=link['url'],
                        title=link['title'],
                        content=text_content[:8000],  # Limite de caractÃ¨res
                        timestamp=datetime.now(),
                        metadata={
                            'word_count': len(text_content.split()),
                            'language': 'fr'  # Ã€ amÃ©liorer avec dÃ©tection automatique
                        }
                    )
                    
                    results.append(scraped_data)
                    
                except Exception as e:
                    logger.error(f"âŒ Erreur scraping {link['url']}: {e}")
                    # Ajouter un rÃ©sultat d'erreur pour cette URL
                    results.append(ScrapedData(
                        url=link['url'],
                        title=link['title'],
                        content=f"Erreur lors du scraping de cette page: {str(e)}",
                        timestamp=datetime.now(),
                        metadata={
                            'word_count': 10,
                            'language': 'fr',
                            'error': str(e)
                        }
                    ))
                    
        finally:
            await page.close()
            
        logger.info(f"âœ… Scraping terminÃ©: {len(results)} pages collectÃ©es")
        return results
    
    def _extract_main_content(self, soup: BeautifulSoup) -> str:
        """Extraction intelligente du contenu principal"""
        # Suppression des Ã©lÃ©ments non pertinents
        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            element.decompose()
            
        # Recherche du contenu principal
        main_content = (
            soup.find('main') or 
            soup.find('article') or 
            soup.find('div', class_='content') or
            soup.find('div', class_='main') or
            soup.find('div', id='content')
        )
        
        if main_content:
            text = main_content.get_text(separator=' ', strip=True)
        else:
            text = soup.get_text(separator=' ', strip=True)
            
        # Nettoyage du texte
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        cleaned_text = ' '.join(lines)
        
        # Suppression des espaces multiples
        import re
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
        
        return cleaned_text
    
    async def process_with_llm(self, prompt: str, scraped_data: List[ScrapedData]) -> Dict:
        """Traite les donnÃ©es scrapÃ©es avec OpenAI"""
        try:
            # PrÃ©paration du contexte
            context = self._prepare_context(scraped_data)
            
            # Prompt systÃ¨me pour structurer la rÃ©ponse
            system_prompt = """
            Tu es un assistant expert en analyse de donnÃ©es web et en intelligence Ã©conomique.
            Analyse les informations fournies et rÃ©ponds Ã  la demande de l'utilisateur.
            
            Structure ta rÃ©ponse de maniÃ¨re claire avec:
            1. Un rÃ©sumÃ© exÃ©cutif (2-3 phrases)
            2. Les points clÃ©s extraits (liste)
            3. Les donnÃ©es structurÃ©es (si applicable)
            4. Les sources utilisÃ©es
            5. Les insights et recommandations
            
            Retourne ta rÃ©ponse en JSON avec la structure suivante:
            {
                "summary": "rÃ©sumÃ© concis",
                "key_points": ["point 1", "point 2", ...],
                "structured_data": {},
                "insights": ["insight 1", "insight 2", ...],
                "recommendations": ["recommandation 1", "recommandation 2", ...],
                "sources": [{"title": "", "url": ""}],
                "confidence_score": 0.85
            }
            """
            
            from os import getenv
            model_name = getenv("AI_MODEL", "gpt-5-thinking")
            from gpt5_compat import from_chat_completions_compat
            response = from_chat_completions_compat(
                client=self.openai_client,
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Demande: {prompt}\n\nDonnÃ©es collectÃ©es:\n{context}"}
                ],
                response_format={"type": "json_object"},
                max_tokens=2000
            )
            
            result = json.loads(response.choices[0].message.content)
            logger.info("âœ… Traitement LLM terminÃ©")
            return result
            
        except Exception as e:
            logger.error(f"âŒ Erreur traitement LLM: {e}")
            return {
                "summary": "Erreur lors du traitement",
                "key_points": [],
                "structured_data": {},
                "insights": [],
                "recommendations": [],
                "sources": [],
                "confidence_score": 0.0,
                "error": str(e)
            }
    
    def _prepare_context(self, scraped_data: List[ScrapedData]) -> str:
        """PrÃ©pare le contexte pour le LLM"""
        context_parts = []
        for idx, data in enumerate(scraped_data, 1):
            context_parts.append(f"""
            Source {idx}: {data.title}
            URL: {data.url}
            Contenu: {data.content[:3000]}
            MÃ©tadonnÃ©es: {data.metadata}
            ---
            """)
        return '\n'.join(context_parts)
    
    async def execute_scraping_task(self, task_id: str) -> Dict:
        """ExÃ©cute une tÃ¢che de scraping complÃ¨te"""
        if task_id not in self.tasks:
            raise ValueError(f"TÃ¢che {task_id} non trouvÃ©e")
            
        task = self.tasks[task_id]
        task.status = "processing"
        
        try:
            logger.info(f"ğŸš€ DÃ©but exÃ©cution tÃ¢che: {task_id}")
            
            # S'assurer que le navigateur est initialisÃ©
            if not self._initialized or not self.browser:
                await self.initialize()
            
            # Scraping
            scraped_data = await self.search_and_scrape(task.prompt, num_results=5)
            
            if not scraped_data:
                task.status = "failed"
                task.error = "Aucune donnÃ©e scrapÃ©e"
                return {"error": "Aucune donnÃ©e trouvÃ©e"}
            
            # Traitement LLM
            processed_result = await self.process_with_llm(task.prompt, scraped_data)
            
            # Mise Ã  jour de la tÃ¢che
            task.status = "completed"
            task.results = processed_result
            task.completed_at = datetime.now()
            
            logger.info(f"âœ… TÃ¢che {task_id} terminÃ©e avec succÃ¨s")
            return processed_result
            
        except Exception as e:
            task.status = "failed"
            task.error = str(e)
            task.completed_at = datetime.now()
            logger.error(f"âŒ Erreur tÃ¢che {task_id}: {e}")
            return {"error": str(e)}
    
    async def get_task_status(self, task_id: str) -> Optional[ScrapingTask]:
        """RÃ©cupÃ¨re le statut d'une tÃ¢che"""
        return self.tasks.get(task_id)
    
    async def cleanup(self):
        """Nettoie les ressources"""
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
        self._initialized = False
        logger.info("ğŸ§¹ Scraper nettoyÃ©")

# Instance globale du scraper
intelligent_scraper = None

async def get_scraper() -> IntelligentScraper:
    """Retourne l'instance globale du scraper"""
    global intelligent_scraper
    if intelligent_scraper is None:
        intelligent_scraper = IntelligentScraper()
        await intelligent_scraper.initialize()
    return intelligent_scraper

def test_scraper():
    """Test du scraper"""
    async def test():
        scraper = await get_scraper()
        
        # Test de scraping
        print("ğŸ§ª Test du scraper intelligent...")
        
        # CrÃ©er une tÃ¢che
        task_id = await scraper.create_scraping_task(
            "Apple stock price today latest news",
            num_results=3
        )
        
        # ExÃ©cuter la tÃ¢che
        result = await scraper.execute_scraping_task(task_id)
        
        print(f"\nğŸ“Š RÃ©sultat du scraping:")
        print(f"   ğŸ“ RÃ©sumÃ©: {result.get('summary', 'N/A')[:200]}...")
        print(f"   ğŸ“ˆ Points clÃ©s: {len(result.get('key_points', []))}")
        print(f"   ğŸ’¡ Insights: {len(result.get('insights', []))}")
        print(f"   ğŸ¯ Recommandations: {len(result.get('recommendations', []))}")
        print(f"   ğŸ“š Sources: {len(result.get('sources', []))}")
        print(f"   ğŸ¯ Score de confiance: {result.get('confidence_score', 0)}")
        
        await scraper.cleanup()
    
    asyncio.run(test())

if __name__ == "__main__":
    test_scraper() 