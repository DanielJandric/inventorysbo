#!/usr/bin/env python3
"""
Scraper intelligent pour collecter et analyser des donn√©es web
"""
import os
import asyncio
import json
import uuid
from datetime import datetime
from typing import List, Dict, Optional
from dataclasses import dataclass
from dotenv import load_dotenv
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
from openai import OpenAI
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class ScrapedData:
    """Donn√©es scrap√©es d'une page web"""
    url: str
    title: str
    content: str
    timestamp: datetime
    metadata: Dict

@dataclass
class ScrapingTask:
    """T√¢che de scraping"""
    id: str
    prompt: str
    status: str  # pending, processing, completed, failed
    results: Optional[Dict] = None
    created_at: datetime = None
    completed_at: Optional[datetime] = None
    error: Optional[str] = None

class IntelligentScraper:
    """Scraper intelligent utilisant Playwright et OpenAI"""
    
    def __init__(self):
        load_dotenv()
        self.playwright = None
        self.browser = None
        self.openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        self.tasks = {}  # Stockage temporaire des t√¢ches
        self._initialized = False
        
    def initialize_sync(self):
        """Initialise le navigateur Playwright de mani√®re synchrone"""
        try:
            if not self._initialized:
                # Utiliser asyncio.run pour initialiser de mani√®re synchrone
                asyncio.run(self._async_initialize())
                self._initialized = True
                logger.info("‚úÖ Scraper intelligent initialis√© (synchrone)")
        except Exception as e:
            logger.error(f"‚ùå Erreur initialisation scraper synchrone: {e}")
            raise
    
    async def _async_initialize(self):
        """Initialisation asynchrone interne"""
        try:
            self.playwright = await async_playwright().start()
            self.browser = await self.playwright.chromium.launch(
                headless=True,
                args=['--no-sandbox', '--disable-dev-shm-usage', '--disable-gpu']
            )
            logger.info("‚úÖ Navigateur Playwright initialis√©")
        except Exception as e:
            logger.error(f"‚ùå Erreur initialisation navigateur: {e}")
            raise
    
    async def initialize(self):
        """Initialise le navigateur Playwright (m√©thode asynchrone)"""
        if not self._initialized:
            await self._async_initialize()
            self._initialized = True
            logger.info("‚úÖ Scraper intelligent initialis√©")
    
    async def create_scraping_task(self, prompt: str, num_results: int = 5) -> str:
        """Cr√©e une nouvelle t√¢che de scraping"""
        task_id = str(uuid.uuid4())
        task = ScrapingTask(
            id=task_id,
            prompt=prompt,
            status="pending",
            created_at=datetime.now()
        )
        self.tasks[task_id] = task
        logger.info(f"üìã T√¢che de scraping cr√©√©e: {task_id}")
        return task_id
    
    async def search_and_scrape(self, query: str, num_results: int = 5) -> List[ScrapedData]:
        """Recherche sur Google et scrape les r√©sultats pertinents"""
        # S'assurer que le navigateur est initialis√©
        if not self._initialized or not self.browser:
            await self.initialize()
            
        page = await self.browser.new_page()
        results = []
        
        try:
            # Recherche Google
            search_url = f"https://www.google.com/search?q={query}"
            logger.info(f"üîç Recherche: {query}")
            
            await page.goto(search_url, wait_until='domcontentloaded')
            
            # Attendre que la page soit charg√©e avec des s√©lecteurs alternatifs
            selectors_to_try = [
                'h3',
                'div[data-sokoban-container] h3',
                'div[jscontroller] h3',
                'a[href] h3',
                'div[role="heading"]',
                'h1, h2, h3'
            ]
            
            page_loaded = False
            for selector in selectors_to_try:
                try:
                    await page.wait_for_selector(selector, timeout=5000)
                    page_loaded = True
                    logger.info(f"‚úÖ S√©lecteur trouv√©: {selector}")
                    break
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è S√©lecteur {selector} non trouv√©: {e}")
                    continue
            
            if not page_loaded:
                logger.warning("‚ö†Ô∏è Aucun s√©lecteur trouv√©, tentative de scraping direct")
                # Attendre un peu et essayer de continuer
                await page.wait_for_timeout(3000)
            
            # Extraction des liens avec plusieurs m√©thodes
            links = []
            
            # M√©thode 1: S√©lecteur h3 dans les liens
            try:
                links = await page.evaluate("""
                    () => {
                        const links = [];
                        document.querySelectorAll('a h3').forEach((h3) => {
                            const link = h3.closest('a');
                            if (link && link.href && !link.href.includes('google.com')) {
                                links.push({
                                    url: link.href,
                                    title: h3.innerText
                                });
                            }
                        });
                        return links.slice(0, 5);
                    }
                """)
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è M√©thode 1 √©chou√©e: {e}")
            
            # M√©thode 2: Recherche plus large si la premi√®re √©choue
            if not links:
                try:
                    links = await page.evaluate("""
                        () => {
                            const links = [];
                            document.querySelectorAll('a[href]').forEach((link) => {
                                if (link.href && 
                                    !link.href.includes('google.com') && 
                                    !link.href.includes('youtube.com') &&
                                    link.innerText.trim().length > 10) {
                                    const title = link.querySelector('h3, h2, div')?.innerText || link.innerText;
                                    if (title && title.trim().length > 5) {
                                        links.push({
                                            url: link.href,
                                            title: title.trim()
                                        });
                                    }
                                }
                            });
                            return links.slice(0, 5);
                        }
                    """)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è M√©thode 2 √©chou√©e: {e}")
            
            # M√©thode 3: Recherche par texte si les autres √©chouent
            if not links:
                try:
                    links = await page.evaluate("""
                        () => {
                            const links = [];
                            const allLinks = Array.from(document.querySelectorAll('a[href]'));
                            const validLinks = allLinks.filter(link => 
                                link.href && 
                                !link.href.includes('google.com') && 
                                !link.href.includes('youtube.com') &&
                                link.innerText.trim().length > 10
                            );
                            
                            validLinks.slice(0, 5).forEach(link => {
                                links.push({
                                    url: link.href,
                                    title: link.innerText.trim().substring(0, 100)
                                });
                            });
                            return links;
                        }
                    """)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è M√©thode 3 √©chou√©e: {e}")
            
            logger.info(f"üìÑ {len(links)} liens trouv√©s")
            
            # Si aucun lien trouv√©, cr√©er un r√©sultat factice pour √©viter l'erreur
            if not links:
                logger.warning("‚ö†Ô∏è Aucun lien trouv√©, cr√©ation d'un r√©sultat factice")
                results.append(ScrapedData(
                    url="https://example.com",
                    title="Aucun r√©sultat trouv√©",
                    content=f"Recherche pour '{query}' - Aucun r√©sultat disponible. Cela peut √™tre d√ª √† des restrictions de Google ou √† un probl√®me de r√©seau.",
                    timestamp=datetime.now(),
                    metadata={
                        'word_count': 20,
                        'language': 'fr',
                        'note': 'R√©sultat factice - aucun lien trouv√©'
                    }
                ))
                return results
            
            # Scraping de chaque page
            for i, link in enumerate(links[:num_results]):
                try:
                    logger.info(f"üìñ Scraping {i+1}/{len(links)}: {link['title']}")
                    await page.goto(link['url'], timeout=15000, wait_until='domcontentloaded')
                    content = await page.content()
                    soup = BeautifulSoup(content, 'html.parser')
                    
                    # Extraction intelligente du contenu
                    text_content = self._extract_main_content(soup)
                    
                    scraped_data = ScrapedData(
                        url=link['url'],
                        title=link['title'],
                        content=text_content[:8000],  # Limite de caract√®res
                        timestamp=datetime.now(),
                        metadata={
                            'word_count': len(text_content.split()),
                            'language': 'fr'  # √Ä am√©liorer avec d√©tection automatique
                        }
                    )
                    
                    results.append(scraped_data)
                    
                except Exception as e:
                    logger.error(f"‚ùå Erreur scraping {link['url']}: {e}")
                    # Ajouter un r√©sultat d'erreur pour cette URL
                    results.append(ScrapedData(
                        url=link['url'],
                        title=link['title'],
                        content=f"Erreur lors du scraping de cette page: {str(e)}",
                        timestamp=datetime.now(),
                        metadata={
                            'word_count': 10,
                            'language': 'fr',
                            'error': str(e)
                        }
                    ))
                    
        finally:
            await page.close()
            
        logger.info(f"‚úÖ Scraping termin√©: {len(results)} pages collect√©es")
        return results
    
    def _extract_main_content(self, soup: BeautifulSoup) -> str:
        """Extraction intelligente du contenu principal"""
        # Suppression des √©l√©ments non pertinents
        for element in soup(['script', 'style', 'nav', 'footer', 'header', 'aside']):
            element.decompose()
            
        # Recherche du contenu principal
        main_content = (
            soup.find('main') or 
            soup.find('article') or 
            soup.find('div', class_='content') or
            soup.find('div', class_='main') or
            soup.find('div', id='content')
        )
        
        if main_content:
            text = main_content.get_text(separator=' ', strip=True)
        else:
            text = soup.get_text(separator=' ', strip=True)
            
        # Nettoyage du texte
        lines = [line.strip() for line in text.split('\n') if line.strip()]
        cleaned_text = ' '.join(lines)
        
        # Suppression des espaces multiples
        import re
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
        
        return cleaned_text
    
    async def process_with_llm(self, prompt: str, scraped_data: List[ScrapedData]) -> Dict:
        """Traite les donn√©es scrap√©es avec OpenAI"""
        try:
            # Pr√©paration du contexte
            context = self._prepare_context(scraped_data)
            
            # Prompt syst√®me pour structurer la r√©ponse
            system_prompt = """
            Tu es un assistant expert en analyse de donn√©es web et en intelligence √©conomique.
            Analyse les informations fournies et r√©ponds √† la demande de l'utilisateur.
            
            Structure ta r√©ponse de mani√®re claire avec:
            1. Un r√©sum√© ex√©cutif (2-3 phrases)
            2. Les points cl√©s extraits (liste)
            3. Les donn√©es structur√©es (si applicable)
            4. Les sources utilis√©es
            5. Les insights et recommandations
            
            Retourne ta r√©ponse en JSON avec la structure suivante:
            {
                "summary": "r√©sum√© concis",
                "key_points": ["point 1", "point 2", ...],
                "structured_data": {},
                "insights": ["insight 1", "insight 2", ...],
                "recommendations": ["recommandation 1", "recommandation 2", ...],
                "sources": [{"title": "", "url": ""}],
                "confidence_score": 0.85
            }
            """
            
            from os import getenv
            model_name = getenv("AI_MODEL", "gpt-5-thinking")
            from gpt5_compat import from_chat_completions_compat
            response = from_chat_completions_compat(
                client=self.openai_client,
                model=model_name,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Demande: {prompt}\n\nDonn√©es collect√©es:\n{context}"}
                ],
                response_format={"type": "json_object"},
                max_tokens=2000
            )
            
            result = json.loads(response.choices[0].message.content)
            logger.info("‚úÖ Traitement LLM termin√©")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erreur traitement LLM: {e}")
            return {
                "summary": "Erreur lors du traitement",
                "key_points": [],
                "structured_data": {},
                "insights": [],
                "recommendations": [],
                "sources": [],
                "confidence_score": 0.0,
                "error": str(e)
            }
    
    def _prepare_context(self, scraped_data: List[ScrapedData]) -> str:
        """Pr√©pare le contexte pour le LLM"""
        context_parts = []
        for idx, data in enumerate(scraped_data, 1):
            context_parts.append(f"""
            Source {idx}: {data.title}
            URL: {data.url}
            Contenu: {data.content[:3000]}
            M√©tadonn√©es: {data.metadata}
            ---
            """)
        return '\n'.join(context_parts)
    
    async def execute_scraping_task(self, task_id: str) -> Dict:
        """Ex√©cute une t√¢che de scraping compl√®te"""
        if task_id not in self.tasks:
            raise ValueError(f"T√¢che {task_id} non trouv√©e")
            
        task = self.tasks[task_id]
        task.status = "processing"
        
        try:
            logger.info(f"üöÄ D√©but ex√©cution t√¢che: {task_id}")
            
            # S'assurer que le navigateur est initialis√©
            if not self._initialized or not self.browser:
                await self.initialize()
            
            # Scraping
            scraped_data = await self.search_and_scrape(task.prompt, num_results=5)
            
            if not scraped_data:
                task.status = "failed"
                task.error = "Aucune donn√©e scrap√©e"
                return {"error": "Aucune donn√©e trouv√©e"}
            
            # Traitement LLM
            processed_result = await self.process_with_llm(task.prompt, scraped_data)
            
            # Mise √† jour de la t√¢che
            task.status = "completed"
            task.results = processed_result
            task.completed_at = datetime.now()
            
            logger.info(f"‚úÖ T√¢che {task_id} termin√©e avec succ√®s")
            return processed_result
            
        except Exception as e:
            task.status = "failed"
            task.error = str(e)
            task.completed_at = datetime.now()
            logger.error(f"‚ùå Erreur t√¢che {task_id}: {e}")
            return {"error": str(e)}
    
    async def get_task_status(self, task_id: str) -> Optional[ScrapingTask]:
        """R√©cup√®re le statut d'une t√¢che"""
        return self.tasks.get(task_id)
    
    async def cleanup(self):
        """Nettoie les ressources"""
        if self.browser:
            await self.browser.close()
        if self.playwright:
            await self.playwright.stop()
        self._initialized = False
        logger.info("üßπ Scraper nettoy√©")

# Instance globale du scraper
intelligent_scraper = None

async def get_scraper() -> IntelligentScraper:
    """Retourne l'instance globale du scraper"""
    global intelligent_scraper
    if intelligent_scraper is None:
        intelligent_scraper = IntelligentScraper()
        await intelligent_scraper.initialize()
    return intelligent_scraper

def test_scraper():
    """Test du scraper"""
    async def test():
        scraper = await get_scraper()
        
        # Test de scraping
        print("üß™ Test du scraper intelligent...")
        
        # Cr√©er une t√¢che
        task_id = await scraper.create_scraping_task(
            "Apple stock price today latest news",
            num_results=3
        )
        
        # Ex√©cuter la t√¢che
        result = await scraper.execute_scraping_task(task_id)
        
        print(f"\nüìä R√©sultat du scraping:")
        print(f"   üìù R√©sum√©: {result.get('summary', 'N/A')[:200]}...")
        print(f"   üìà Points cl√©s: {len(result.get('key_points', []))}")
        print(f"   üí° Insights: {len(result.get('insights', []))}")
        print(f"   üéØ Recommandations: {len(result.get('recommendations', []))}")
        print(f"   üìö Sources: {len(result.get('sources', []))}")
        print(f"   üéØ Score de confiance: {result.get('confidence_score', 0)}")
        
        await scraper.cleanup()
    
    asyncio.run(test())

if __name__ == "__main__":
    test_scraper() 