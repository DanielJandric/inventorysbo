import os
import asyncio
import aiohttp
import json
import logging
from datetime import datetime
from typing import List, Dict, Optional
from dataclasses import dataclass
from urllib.parse import quote_plus
import re

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Charger les variables d'environnement de mani√®re s√©curis√©e
try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Impossible de charger .env: {e}")

@dataclass
class ScrapedData:
    url: str
    title: str
    content: str
    timestamp: datetime
    metadata: Dict

@dataclass
class ScrapingTask:
    id: str
    prompt: str
    status: str  # pending, processing, completed, failed
    results: Optional[Dict] = None
    created_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    error: Optional[str] = None

class ScrapingBeeScraper:
    def __init__(self):
        self.api_key = os.getenv('SCRAPINGBEE_API_KEY')
        self.base_url = "https://app.scrapingbee.com/api/v1"
        self.tasks = {}
        self._initialized = False
        
        if not self.api_key:
            logger.warning("‚ö†Ô∏è SCRAPINGBEE_API_KEY non configur√©")
        
    def initialize_sync(self):
        """Initialisation synchrone du scraper"""
        try:
            if not self._initialized:
                if not self.api_key:
                    raise Exception("SCRAPINGBEE_API_KEY requis")
                
                self._initialized = True
                logger.info("‚úÖ ScrapingBee Scraper initialis√©")
        except Exception as e:
            logger.error(f"‚ùå Erreur initialisation ScrapingBee: {e}")
            raise
    
    async def create_scraping_task(self, prompt: str, num_results: int = 5) -> str:
        """Cr√©e une nouvelle t√¢che de scraping"""
        import uuid
        task_id = str(uuid.uuid4())
        
        task = ScrapingTask(
            id=task_id,
            prompt=prompt,
            status="pending",
            created_at=datetime.now()
        )
        
        self.tasks[task_id] = task
        logger.info(f"üìã T√¢che de scraping cr√©√©e: {task_id}")
        return task_id
    
    async def search_and_scrape(self, query: str, num_results: int = 5) -> List[ScrapedData]:
        """Recherche sur Google et scrape les r√©sultats avec ScrapingBee"""
        if not self._initialized:
            await self.initialize()
        
        results = []
        
        try:
            logger.info(f"üîç Recherche ScrapingBee: {query}")
            
            # √âtape 1: Recherche Google avec ScrapingBee
            search_results = await self._search_google(query, num_results)
            
            if not search_results:
                logger.warning("‚ö†Ô∏è Aucun r√©sultat de recherche trouv√©")
                return results
            
            # √âtape 2: Scraping des pages avec ScrapingBee
            for i, result in enumerate(search_results[:num_results]):
                try:
                    logger.info(f"üìñ Scraping {i+1}/{len(search_results)}: {result['title']}")
                    
                    scraped_content = await self._scrape_page(result['url'])
                    
                    if scraped_content:
                        results.append(ScrapedData(
                            url=result['url'],
                            title=result['title'],
                            content=scraped_content,
                            timestamp=datetime.now(),
                            metadata={
                                'word_count': len(scraped_content.split()),
                                'language': 'fr',
                                'source': 'scrapingbee'
                            }
                        ))
                    
                except Exception as e:
                    logger.error(f"‚ùå Erreur scraping {result['url']}: {e}")
                    # Ajouter un r√©sultat d'erreur
                    results.append(ScrapedData(
                        url=result['url'],
                        title=result['title'],
                        content=f"Erreur lors du scraping: {str(e)}",
                        timestamp=datetime.now(),
                        metadata={
                            'word_count': 10,
                            'language': 'fr',
                            'error': str(e)
                        }
                    ))
            
        except Exception as e:
            logger.error(f"‚ùå Erreur recherche/scraping: {e}")
        
        return results
    
    async def _search_google(self, query: str, num_results: int = 5) -> List[Dict]:
        """Recherche sur des sites financiers directs avec ScrapingBee"""
        try:
            # D√©tecter si c'est une requ√™te sur les march√©s g√©n√©raux ou une action sp√©cifique
            query_lower = query.lower()
            
            if any(word in query_lower for word in ['march√©', 'march√©s', 'financier', 'financiers', 'situation', 'aujourd\'hui', 'ia', 'ai', 'intelligence']):
                # Sites d'actualit√©s financi√®res pour les requ√™tes g√©n√©rales
                sites_financiers = [
                    {
                        'url': "https://www.reuters.com/markets/",
                        'title': "Reuters - March√©s financiers"
                    },
                    {
                        'url': "https://www.bloomberg.com/markets",
                        'title': "Bloomberg - March√©s"
                    },
                    {
                        'url': "https://www.ft.com/markets",
                        'title': "Financial Times - March√©s"
                    },
                    {
                        'url': "https://www.cnbc.com/markets/",
                        'title': "CNBC - March√©s"
                    },
                    {
                        'url': "https://www.marketwatch.com/",
                        'title': "MarketWatch - Actualit√©s"
                    }
                ]
            else:
                # Sites de stocks pour les requ√™tes sp√©cifiques
                stock_symbol = query.split()[0].upper()
                sites_financiers = [
                    {
                        'url': f"https://finance.yahoo.com/quote/{stock_symbol}",
                        'title': f"Yahoo Finance - {stock_symbol}"
                    },
                    {
                        'url': f"https://www.marketwatch.com/investing/stock/{stock_symbol.lower()}",
                        'title': f"MarketWatch - {stock_symbol}"
                    },
                    {
                        'url': f"https://www.reuters.com/companies/{stock_symbol}.O",
                        'title': f"Reuters - {stock_symbol}"
                    },
                    {
                        'url': f"https://www.bloomberg.com/quote/{stock_symbol}:US",
                        'title': f"Bloomberg - {stock_symbol}"
                    }
                ]
            
            logger.info(f"üîç Recherche sur {min(len(sites_financiers), num_results)} sites financiers")
            
            # Retourner les sites financiers comme r√©sultats de recherche
            return sites_financiers[:num_results]
                        
        except Exception as e:
            logger.error(f"‚ùå Erreur recherche sites financiers: {e}")
            return []  # Pas de fallback
    
    def _extract_links_from_html(self, html_content: str, query: str) -> List[Dict]:
        """Extrait les liens depuis le HTML de Google"""
        import re
        
        links = []
        
        # Patterns am√©lior√©s pour extraire les liens des r√©sultats Google
        patterns = [
            # Pattern pour les r√©sultats de recherche Google standard
            r'<a[^>]*href="([^"]*)"[^>]*>([^<]*)</a>',
            # Pattern pour les titres de r√©sultats
            r'<h3[^>]*><a[^>]*href="([^"]*)"[^>]*>([^<]*)</a></h3>',
            # Pattern pour les divs avec titres
            r'<div[^>]*class="[^"]*title[^"]*"[^>]*><a[^>]*href="([^"]*)"[^>]*>([^<]*)</a></div>',
            # Pattern pour les liens dans les r√©sultats de recherche
            r'<a[^>]*href="([^"]*)"[^>]*class="[^"]*"[^>]*>([^<]*)</a>',
            # Pattern g√©n√©rique pour tous les liens
            r'<a[^>]*href="([^"]*)"[^>]*>([^<]*)</a>'
        ]
        
        logger.info(f"üîç Extraction de liens depuis {len(html_content)} caract√®res")
        
        for i, pattern in enumerate(patterns):
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            logger.info(f"üîç Pattern {i+1}: {len(matches)} matches trouv√©s")
            
            for url, title in matches:
                logger.debug(f"üîç Match trouv√©: URL='{url[:100]}...' Title='{title[:100]}...'")
                
                # Nettoyer l'URL (enlever les param√®tres Google)
                original_url = url
                if url.startswith('/url?q='):
                    url = url.split('/url?q=')[1].split('&')[0]
                elif url.startswith('/url?'):
                    url = url.split('/url?')[1].split('&')[0]
                
                logger.debug(f"üîç URL nettoy√©e: {url[:100]}...")
                
                # V√©rifier si c'est un lien valide
                is_valid = True
                reason = ""
                
                if not url.startswith('http'):
                    is_valid = False
                    reason = "Pas HTTP"
                elif url.startswith('https://www.google.com'):
                    is_valid = False
                    reason = "Google.com"
                elif url.startswith('https://accounts.google.com'):
                    is_valid = False
                    reason = "Accounts Google"
                elif url.startswith('https://maps.google.com'):
                    is_valid = False
                    reason = "Maps Google"
                elif url.startswith('https://support.google.com'):
                    is_valid = False
                    reason = "Support Google"
                elif len(title.strip()) <= 5:
                    is_valid = False
                    reason = "Titre trop court"
                elif any(domain in url.lower() for domain in ['google.com', 'youtube.com', 'facebook.com', 'instagram.com']):
                    is_valid = False
                    reason = "Domaine exclu"
                
                if is_valid:
                    # Nettoyer le titre
                    title = re.sub(r'<[^>]+>', '', title).strip()
                    
                    if title and len(title) > 5:
                        links.append({
                            'url': url,
                            'title': title
                        })
                        logger.info(f"‚úÖ Lien trouv√©: {title[:50]}... -> {url}")
                    else:
                        logger.debug(f"‚ùå Titre invalide apr√®s nettoyage: '{title}'")
                else:
                    logger.debug(f"‚ùå Lien rejet√© ({reason}): {url[:50]}...")
        
        # Supprimer les doublons
        seen_urls = set()
        unique_links = []
        for link in links:
            if link['url'] not in seen_urls:
                seen_urls.add(link['url'])
                unique_links.append(link)
        
        logger.info(f"üìÑ Total: {len(unique_links)} liens uniques extraits")
        return unique_links
    

    
    async def _scrape_page(self, url: str) -> Optional[str]:
        """Scrape une page avec ScrapingBee"""
        try:
            # Param√®tres ScrapingBee pour le scraping
            params = {
                'api_key': self.api_key,
                'url': url,
                'render_js': 'true',  # Rendu JS pour les pages dynamiques
                'premium_proxy': 'false',  # Proxy standard pour √©conomiser les cr√©dits
                'country_code': 'us'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(self.base_url, params=params) as response:
                    if response.status == 200:
                        # ScrapingBee retourne du HTML, pas du JSON
                        html_content = await response.text()
                        
                        # Extraire le contenu du body
                        cleaned_content = self._extract_text_from_html(html_content)
                        logger.info(f"üìÑ Contenu extrait de {url}: {len(cleaned_content)} caract√®res.")
                        logger.debug(f"Contenu brut (aper√ßu): {cleaned_content[:500]}...")

                        return cleaned_content[:8000]  # Limite de caract√®res augment√©e
                    else:
                        logger.error(f"‚ùå Erreur ScrapingBee scraping: {response.status}")
                        return None
                        
        except Exception as e:
            logger.error(f"‚ùå Erreur scraping page {url}: {e}")
            return None
    
    def _extract_text_from_html(self, html_content: str) -> str:
        """Extrait le texte du HTML"""
        if not html_content:
            return ""
        
        # Supprimer les balises script et style
        html_content = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL)
        html_content = re.sub(r'<style[^>]*>.*?</style>', '', html_content, flags=re.DOTALL)
        
        # Extraire le texte des balises
        text = re.sub(r'<[^>]+>', ' ', html_content)
        
        # Nettoyer le texte
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s\.\,\!\?\-\:\;\(\)\-\$\%]', '', text)
        
        return text.strip()[:15000]
    
    async def _scrape_with_params(self, url: str, params: Dict) -> Optional[str]:
        """Scrape une page avec des param√®tres ScrapingBee sp√©cifiques."""
        try:
            # Les param√®tres de base sont fusionn√©s avec les param√®tres sp√©cifiques
            base_params = {
                'api_key': self.api_key,
                'url': url,
            }
            final_params = {**base_params, **params}

            async with aiohttp.ClientSession() as session:
                async with session.get(self.base_url, params=final_params) as response:
                    if response.status == 200:
                        return await response.text()
                    else:
                        response_text = await response.text()
                        logger.error(f"‚ùå Erreur ScrapingBee avec params: {response.status}")
                        logger.error(f"URL: {url}")
                        logger.error(f"Params: {params}")
                        logger.error(f"R√©ponse de ScrapingBee: {response_text}")
                        return None
        except Exception as e:
            logger.error(f"‚ùå Erreur scraping page avec params {url}: {e}")
            return None

    
    def _clean_content(self, content: str) -> str:
        """Nettoie le contenu extrait"""
        if not content:
            return ""
        
        # Supprimer les espaces multiples
        content = re.sub(r'\s+', ' ', content)
        
        # Supprimer les caract√®res sp√©ciaux
        content = re.sub(r'[^\w\s\.\,\!\?\-\:\;\(\)]', '', content)
        
        # Limiter la longueur
        return content.strip()[:15000]
    
    async def process_with_llm(self, prompt: str, scraped_data: List[ScrapedData], market_snapshot: Dict) -> Dict:
        """Traite les donn√©es scrap√©es avec OpenAI"""
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
            
            # Pr√©parer le contexte
            context = self._prepare_context(scraped_data)
            logger.info(f"üß† Contexte pr√©par√© pour OpenAI ({len(context)} caract√®res).")
            logger.info(f"üìä Nombre de sources: {len(scraped_data)}")
            logger.info(f"üìà Market snapshot disponible: {'Oui' if market_snapshot else 'Non'}")
            logger.debug(f"Contexte complet pour OpenAI: {context}")

            # Prompt syst√®me enrichi
            system_prompt = """Tu es un expert analyste financier et g√©opolitique de classe mondiale. Ta mission est de produire un rapport de march√© EXHAUSTIF int√©grant les dimensions √âCONOMIQUES et G√âOPOLITIQUES. Combine les donn√©es factuelles (market_snapshot) avec l'analyse des textes (donn√©es collect√©es).

STRUCTURE OBLIGATOIRE DE LA R√âPONSE JSON :
{
    "executive_summary": [
        "‚Ä¢ S&P 500: 5447.87 (-0.16%) - Impact de la politique mon√©taire restrictive",
        "‚Ä¢ Bitcoin: $69,304 (+0.65%) - Afflux institutionnel suite aux ETF",
        "‚Ä¢ Tensions g√©opolitiques: Escalade au Moyen-Orient fait grimper le p√©trole √† $85/baril",
        "‚Ä¢ BCE: Maintien des taux √† 4.5% - Inflation zone euro √† 2.9%",
        "‚Ä¢ Tech/IA: NVIDIA +3.2% - Demande IA d√©passe les pr√©visions Q4"
    ],
    "market_snapshot": {
        "indices": {
            "S&P 500": {"price": 5447.87, "change": -8.55, "change_percent": -0.16},
            "NASDAQ": {"price": 17689.36, "change": -28.20, "change_percent": -0.16}
        },
        "commodities": {"Gold": {"price": 2330.20, "change": -1.20, "change_percent": -0.05}},
        "crypto": {"Bitcoin": {"price": 69304.58, "change": 450.15, "change_percent": 0.65}}
    },
    "geopolitical_analysis": {
        "conflicts": ["Conflit/tension actuel et impact sur les march√©s"],
        "trade_relations": ["√âvolutions commerciales majeures"],
        "sanctions": ["Nouvelles sanctions et leurs cons√©quences"],
        "energy_security": ["Enjeux √©nerg√©tiques actuels"]
    },
    "economic_indicators": {
        "inflation": {"US": "3.2%", "EU": "2.9%", "trend": "d√©c√©l√©ration"},
        "central_banks": ["Fed: pause √† 5.5%", "BCE: maintien √† 4.5%"],
        "gdp_growth": {"US": "2.8%", "EU": "0.6%", "China": "5.2%"},
        "unemployment": {"US": "3.7%", "EU": "6.5%"}
    },
    "summary": "Un r√©sum√© ex√©cutif substantiel int√©grant l'analyse √©conomique ET g√©opolitique. Minimum 500 mots.",
    "key_points": [
        "Point cl√© d√©taill√© avec donn√©es chiffr√©es", 
        "...",
        "Minimum 10 points"
    ],
    "structured_data": {
        "market_sentiment": "Analyse du sentiment avec justification √©conomique et g√©opolitique",
        "key_trends": ["Tendance majeure avec impact chiffr√©"],
        "major_events": ["√âv√©nement g√©opolitique/√©conomique et cons√©quences"],
        "sector_analysis": "Analyse sectorielle avec performances chiffr√©es"
    },
    "insights": ["Insight actionnable avec donn√©es quantitatives"],
    "risks": ["Risque g√©opolitique/√©conomique quantifi√©"],
    "opportunities": ["Opportunit√© avec potentiel de rendement"],
    "sources_analysis": "Critique de la fiabilit√© des sources.",
    "confidence_score": 0.95,
    "sources": [{"title": "Titre de la source", "url": "URL"}]
}

IMPORTANT: 
- L'executive_summary doit contenir EXACTEMENT 5 bullet points avec des VALEURS NUM√âRIQUES (prix, pourcentages, montants)
- Int√©grer syst√©matiquement l'analyse G√âOPOLITIQUE et √âCONOMIQUE
- Utiliser des donn√©es chiffr√©es dans CHAQUE section
- Format: "‚Ä¢ [Actif/Th√®me]: [Valeur] ([Variation]) - [Impact/Contexte]" """
            
            model_name = os.getenv("AI_MODEL", "gpt-4.1")
            logger.info(f"ü§ñ Appel √† l'API OpenAI ({model_name}) en cours pour une analyse exhaustive...")
            
            # Essayer jusqu'√† 3 fois en cas d'erreur
            for attempt in range(3):
                try:
                    response = client.chat.completions.create(
                        model=model_name,
                        messages=[
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": f"Demande: {prompt}\n\nDONN√âES FACTUELLES (snapshot):\n{json.dumps(market_snapshot, indent=2)}\n\nDONN√âES COLLECT√âES (articles):\n{context}"}
                        ],
                        response_format={"type": "json_object"},
                        temperature=0.2,
                        max_tokens=4000
                    )
                    
                    result = json.loads(response.choices[0].message.content)
                    logger.info(f"‚úÖ OpenAI a retourn√© une r√©ponse compl√®te")
                    return result
                    
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Tentative {attempt + 1}/3 √©chou√©e: {e}")
                    if attempt < 2:
                        await asyncio.sleep(2)  # Attendre 2 secondes avant de r√©essayer
                    else:
                        raise
            
        except Exception as e:
            logger.error(f"‚ùå Erreur traitement LLM: {e}")
            return {
                "summary": f"Erreur lors du traitement: {str(e)}",
                "key_points": [],
                "structured_data": {},
                "sources": [],
                "confidence_score": 0.0
            }
    
    def _prepare_context(self, scraped_data: List[ScrapedData]) -> str:
        """Pr√©pare le contexte pour le LLM"""
        context_parts = []
        
        for idx, data in enumerate(scraped_data, 1):
            context_parts.append(f"""
Source {idx}: {data.title}
URL: {data.url}
Contenu: {data.content[:8000]}
---
""")
        
        return '\n'.join(context_parts)
    
    async def execute_scraping_task(self, task_id: str) -> Dict:
        """Ex√©cute une t√¢che de scraping"""
        if task_id not in self.tasks:
            return {"error": "T√¢che non trouv√©e"}
        
        task = self.tasks[task_id]
        task.status = "processing"
        
        try:
            logger.info(f"üöÄ D√©but ex√©cution t√¢che: {task_id}")
            
            # Scraping - Utiliser 8 sources pour un rapport vraiment complet
            scraped_data = await self.search_and_scrape(task.prompt, num_results=8)
            
            if not scraped_data:
                task.status = "failed"
                task.error = "Application non disponible - Aucune donn√©e trouv√©e"
                return {
                    "error": "Application non disponible",
                    "message": "Impossible de r√©cup√©rer des donn√©es pour cette requ√™te. Veuillez r√©essayer plus tard.",
                    "status": "unavailable"
                }
            
            # √âtape 2: R√©cup√©rer les donn√©es factuelles de march√©
            from stock_api_manager import stock_api_manager
            market_snapshot = stock_api_manager.get_market_snapshot()

            # √âtape 3: Traitement LLM avec les donn√©es scrap√©es ET les donn√©es factuelles
            llm_result = await self.process_with_llm(task.prompt, scraped_data, market_snapshot)
            
            # Mettre √† jour la t√¢che
            task.status = "completed"
            task.results = llm_result
            task.completed_at = datetime.now()
            
            logger.info(f"‚úÖ T√¢che {task_id} termin√©e avec succ√®s")
            return llm_result
            
        except Exception as e:
            logger.error(f"‚ùå Erreur t√¢che {task_id}: {e}")
            task.status = "failed"
            task.error = str(e)
            return {
                "error": "Application non disponible",
                "message": f"Erreur technique: {str(e)}",
                "status": "error"
            }
    
    def get_task_status(self, task_id: str) -> Optional[ScrapingTask]:
        """R√©cup√®re le statut d'une t√¢che"""
        return self.tasks.get(task_id)
    
    async def initialize(self):
        """Initialisation asynchrone"""
        if not self._initialized:
            self.initialize_sync()
    
    def cleanup(self):
        """Nettoyage des ressources"""
        self.tasks.clear()
        self._initialized = False
        logger.info("üßπ ScrapingBee Scraper nettoy√©")

# Fonction utilitaire pour obtenir le scraper
def get_scrapingbee_scraper():
    """Retourne une instance du ScrapingBee Scraper"""
    return ScrapingBeeScraper()

# Fonction de test
async def test_scrapingbee_scraper():
    """Test du ScrapingBee Scraper"""
    print("üß™ Test du ScrapingBee Scraper")
    print("=" * 50)
    
    scraper = get_scrapingbee_scraper()
    
    try:
        # Test d'initialisation
        scraper.initialize_sync()
        
        # Test de cr√©ation de t√¢che
        print("üìã Test 1: Cr√©ation de t√¢che")
        task_id = await scraper.create_scraping_task("Tesla stock price today latest news earnings", 3)
        print(f"‚úÖ T√¢che cr√©√©e: {task_id}")
        
        # Test d'ex√©cution
        print("üöÄ Test 2: Ex√©cution de la t√¢che")
        result = await scraper.execute_scraping_task(task_id)
        
        if "error" in result:
            print(f"‚ùå Erreur: {result['error']}")
        else:
            print("‚úÖ R√©sultat obtenu:")
            print(json.dumps(result, indent=2, ensure_ascii=False))
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
    finally:
        scraper.cleanup()

if __name__ == "__main__":
    asyncio.run(test_scrapingbee_scraper()) 