import os
import asyncio
import aiohttp
import json
import logging
from datetime import datetime
from typing import List, Dict, Optional
from dataclasses import dataclass
from urllib.parse import quote_plus
import re

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Charger les variables d'environnement de mani√®re s√©curis√©e
try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception as e:
    logger.warning(f"‚ö†Ô∏è Impossible de charger .env: {e}")

@dataclass
class ScrapedData:
    url: str
    title: str
    content: str
    timestamp: datetime
    metadata: Dict

@dataclass
class ScrapingTask:
    id: str
    prompt: str
    status: str  # pending, processing, completed, failed
    results: Optional[Dict] = None
    created_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    error: Optional[str] = None

class ScrapingBeeScraper:
    def __init__(self):
        self.api_key = os.getenv('SCRAPINGBEE_API_KEY')
        self.base_url = "https://app.scrapingbee.com/api/v1"
        self.tasks = {}
        self._initialized = False
        
        if not self.api_key:
            logger.warning("‚ö†Ô∏è SCRAPINGBEE_API_KEY non configur√©")
        
    def initialize_sync(self):
        """Initialisation synchrone du scraper"""
        try:
            if not self._initialized:
                if not self.api_key:
                    raise Exception("SCRAPINGBEE_API_KEY requis")
                
                self._initialized = True
                logger.info("‚úÖ ScrapingBee Scraper initialis√©")
        except Exception as e:
            logger.error(f"‚ùå Erreur initialisation ScrapingBee: {e}")
            raise
    
    async def create_scraping_task(self, prompt: str, num_results: int = 5) -> str:
        """Cr√©e une nouvelle t√¢che de scraping"""
        import uuid
        task_id = str(uuid.uuid4())
        
        task = ScrapingTask(
            id=task_id,
            prompt=prompt,
            status="pending",
            created_at=datetime.now()
        )
        
        self.tasks[task_id] = task
        logger.info(f"üìã T√¢che de scraping cr√©√©e: {task_id}")
        return task_id
    
    async def search_and_scrape(self, query: str, num_results: int = 5) -> List[ScrapedData]:
        """Recherche sur Google et scrape les r√©sultats avec ScrapingBee"""
        if not self._initialized:
            await self.initialize()
        
        results = []
        
        try:
            logger.info(f"üîç Recherche ScrapingBee: {query}")
            
            # √âtape 1: Recherche Google avec ScrapingBee
            search_results = await self._search_google(query, max(num_results, 12))
            
            if not search_results:
                logger.warning("‚ö†Ô∏è Aucun r√©sultat de recherche trouv√©")
                return results
            
            # √âtape 2: Scraping des pages avec ScrapingBee
            for i, result in enumerate(search_results[:max(num_results, 12)]):
                try:
                    logger.info(f"üìñ Scraping {i+1}/{min(len(search_results), max(num_results, 12))}: {result['title']}")
                    
                    scraped = await self._scrape_page(result['url'])
                    
                    if scraped:
                        content = scraped.get('content') if isinstance(scraped, dict) else str(scraped)
                        published_at = scraped.get('published_at') if isinstance(scraped, dict) else None
                        results.append(ScrapedData(
                            url=result['url'],
                            title=result['title'],
                            content=content,
                            timestamp=published_at or datetime.now(),
                            metadata={
                                'word_count': len(content.split()),
                                'published_at': (published_at.isoformat() if published_at else None),
                                'source': 'scrapingbee'
                            }
                        ))
                    
                except Exception as e:
                    logger.error(f"‚ùå Erreur scraping {result['url']}: {e}")
                    results.append(ScrapedData(
                        url=result['url'],
                        title=result['title'],
                        content=f"Erreur lors du scraping: {str(e)}",
                        timestamp=datetime.now(),
                        metadata={'error': str(e), 'source': 'scrapingbee'}
                    ))
            
            # Prioriser les articles r√©cents (quelque soit la source)
            results.sort(key=lambda r: r.timestamp or datetime.min, reverse=True)
            results = results[:max(num_results, 12)]
            
        except Exception as e:
            logger.error(f"‚ùå Erreur recherche/scraping: {e}")
        
        return results
    
    async def _search_google(self, query: str, num_results: int = 5) -> List[Dict]:
        """Recherche sur des sites financiers directs avec ScrapingBee"""
        try:
            q = query.lower()
            # Listes de domaines FR/CH/UK
            fr_sources = [
                ("https://www.lesechos.fr/", "Les Echos"),
                ("https://www.lefigaro.fr/finance/", "Le Figaro Finance"),
                ("https://www.boursorama.com/", "Boursorama"),
                ("https://www.zonebourse.com/", "Zonebourse"),
            ]
            ch_sources = [
                ("https://www.letemps.ch/economie", "Le Temps - Economie"),
                ("https://www.rts.ch/info/economie/", "RTS - Economie"),
                ("https://www.swissinfo.ch/fre/economie", "SWI - Economie"),
            ]
            uk_us_sources = [
                ("https://www.reuters.com/markets/", "Reuters - Markets"),
                ("https://www.ft.com/markets", "FT - Markets"),
                ("https://www.bloomberg.com/markets", "Bloomberg - Markets"),
                ("https://www.cnbc.com/markets/", "CNBC - Markets"),
                ("https://www.marketwatch.com/", "MarketWatch"),
            ]

            sources: List[Dict[str, str]] = []
            # Prioriser FR/CH puis UK/US
            for url, title in fr_sources + ch_sources + uk_us_sources:
                sources.append({'url': url, 'title': title})
            
            logger.info(f"üîç S√©lection de {min(len(sources), num_results)} sources FR/CH/UK prioritaires")
            return sources[:num_results]
        except Exception as e:
            logger.error(f"‚ùå Erreur recherche sites financiers: {e}")
            return []
    
    def _extract_links_from_html(self, html_content: str, query: str) -> List[Dict]:
        """Extrait les liens depuis le HTML de Google"""
        import re
        
        links = []
        
        # Patterns am√©lior√©s pour extraire les liens des r√©sultats Google
        patterns = [
            # Pattern pour les r√©sultats de recherche Google standard
            r'<a[^>]*href="([^"]*)"[^>]*>([^<]*)</a>',
            # Pattern pour les titres de r√©sultats
            r'<h3[^>]*><a[^>]*href="([^"]*)"[^>]*>([^<]*)</a></h3>',
            # Pattern pour les divs avec titres
            r'<div[^>]*class="[^"]*title[^"]*"[^>]*><a[^>]*href="([^"]*)"[^>]*>([^<]*)</a></div>',
            # Pattern pour les liens dans les r√©sultats de recherche
            r'<a[^>]*href="([^"]*)"[^>]*class="[^"]*"[^>]*>([^<]*)</a>',
            # Pattern g√©n√©rique pour tous les liens
            r'<a[^>]*href="([^"]*)"[^>]*>([^<]*)</a>'
        ]
        
        logger.info(f"üîç Extraction de liens depuis {len(html_content)} caract√®res")
        
        for i, pattern in enumerate(patterns):
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            logger.info(f"üîç Pattern {i+1}: {len(matches)} matches trouv√©s")
            
            for url, title in matches:
                logger.debug(f"üîç Match trouv√©: URL='{url[:100]}...' Title='{title[:100]}...'")
                
                # Nettoyer l'URL (enlever les param√®tres Google)
                original_url = url
                if url.startswith('/url?q='):
                    url = url.split('/url?q=')[1].split('&')[0]
                elif url.startswith('/url?'):
                    url = url.split('/url?')[1].split('&')[0]
                
                logger.debug(f"üîç URL nettoy√©e: {url[:100]}...")
                
                # V√©rifier si c'est un lien valide
                is_valid = True
                reason = ""
                
                if not url.startswith('http'):
                    is_valid = False
                    reason = "Pas HTTP"
                elif url.startswith('https://www.google.com'):
                    is_valid = False
                    reason = "Google.com"
                elif url.startswith('https://accounts.google.com'):
                    is_valid = False
                    reason = "Accounts Google"
                elif url.startswith('https://maps.google.com'):
                    is_valid = False
                    reason = "Maps Google"
                elif url.startswith('https://support.google.com'):
                    is_valid = False
                    reason = "Support Google"
                elif len(title.strip()) <= 5:
                    is_valid = False
                    reason = "Titre trop court"
                elif any(domain in url.lower() for domain in ['google.com', 'youtube.com', 'facebook.com', 'instagram.com']):
                    is_valid = False
                    reason = "Domaine exclu"
                
                if is_valid:
                    # Nettoyer le titre
                    title = re.sub(r'<[^>]+>', '', title).strip()
                    
                    if title and len(title) > 5:
                        links.append({
                            'url': url,
                            'title': title
                        })
                        logger.info(f"‚úÖ Lien trouv√©: {title[:50]}... -> {url}")
                    else:
                        logger.debug(f"‚ùå Titre invalide apr√®s nettoyage: '{title}'")
                else:
                    logger.debug(f"‚ùå Lien rejet√© ({reason}): {url[:50]}...")
        
        # Supprimer les doublons
        seen_urls = set()
        unique_links = []
        for link in links:
            if link['url'] not in seen_urls:
                seen_urls.add(link['url'])
                unique_links.append(link)
        
        logger.info(f"üìÑ Total: {len(unique_links)} liens uniques extraits")
        return unique_links
    

    
    def _extract_published_datetime(self, html_content: str, headers: Dict[str, str]) -> Optional[datetime]:
        """Essaie d'extraire la date de publication depuis HTML/meta ou headers."""
        try:
            # 1) Meta tags courants
            patterns = [
                r'property=["\']article:published_time["\']\s+content=["\']([^"\']+)["\']',
                r'property=["\']og:updated_time["\']\s+content=["\']([^"\']+)["\']',
                r'itemprop=["\']datePublished["\']\s+content=["\']([^"\']+)["\']',
                r'name=["\']date["\']\s+content=["\']([^"\']+)["\']',
                r'name=["\']pubdate["\']\s+content=["\']([^"\']+)["\']',
            ]
            for pat in patterns:
                m = re.search(pat, html_content, flags=re.IGNORECASE)
                if m:
                    val = m.group(1)
                    try:
                        # ISO 8601 simple
                        v = val.replace('Z', '+00:00')
                        return datetime.fromisoformat(v)
                    except Exception:
                        # YYYY-MM-DD HH:MM(:SS)?
                        m2 = re.search(r'(\d{4}-\d{2}-\d{2}[ T]\d{2}:\d{2}(?::\d{2})?)', val)
                        if m2:
                            try:
                                v2 = m2.group(1).replace(' ', 'T')
                                return datetime.fromisoformat(v2)
                            except Exception:
                                pass
            # 2) JSON-LD script
            mld = re.search(r'<script[^>]*type=["\']application/ld\+json["\'][^>]*>(.*?)</script>', html_content, flags=re.IGNORECASE|re.DOTALL)
            if mld:
                try:
                    import json as _json
                    data = _json.loads(mld.group(1))
                    if isinstance(data, dict):
                        val = data.get('datePublished') or data.get('dateModified')
                        if isinstance(val, str):
                            v = val.replace('Z', '+00:00')
                            return datetime.fromisoformat(v)
                except Exception:
                    pass
            # 3) Header Last-Modified
            if headers:
                lm = headers.get('Last-Modified') or headers.get('last-modified')
                if lm:
                    try:
                        from email.utils import parsedate_to_datetime
                        return parsedate_to_datetime(lm)
                    except Exception:
                        pass
        except Exception:
            return None
        return None

    async def _scrape_page(self, url: str) -> Optional[Dict[str, object]]:
        """Scrape une page avec ScrapingBee et retourne contenu + date publi√©e si trouv√©e."""
        try:
            params = {
                'api_key': self.api_key,
                'url': url,
                'render_js': 'true',
                'premium_proxy': 'true',
                'block_resources': 'false',
            }
            async with aiohttp.ClientSession() as session:
                async with session.get(self.base_url, params=params) as response:
                    if response.status == 200:
                        html_content = await response.text()
                        cleaned_content = self._extract_text_from_html(html_content)
                        pub_dt = self._extract_published_datetime(html_content, dict(response.headers))
                        return {"content": cleaned_content[:12000], "published_at": pub_dt}
                    else:
                        logger.error(f"‚ùå Erreur ScrapingBee scraping: {response.status}")
                        return None
        except Exception as e:
            logger.error(f"‚ùå Erreur scraping page {url}: {e}")
            return None
    
    def _extract_text_from_html(self, html_content: str) -> str:
        """Extrait le texte du HTML"""
        if not html_content:
            return ""
        
        # Supprimer les balises script et style
        html_content = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL)
        html_content = re.sub(r'<style[^>]*>.*?</style>', '', html_content, flags=re.DOTALL)
        
        # Extraire le texte des balises
        text = re.sub(r'<[^>]+>', ' ', html_content)
        
        # Nettoyer le texte
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s\.\,\!\?\-\:\;\(\)\-\$\%]', '', text)
        
        return text.strip()[:15000]
    
    async def _scrape_with_params(self, url: str, params: Dict) -> Optional[str]:
        """Scrape une page avec des param√®tres ScrapingBee sp√©cifiques."""
        try:
            # Les param√®tres de base sont fusionn√©s avec les param√®tres sp√©cifiques
            base_params = {
                'api_key': self.api_key,
                'url': url,
            }
            final_params = {**base_params, **params}

            async with aiohttp.ClientSession() as session:
                async with session.get(self.base_url, params=final_params) as response:
                    if response.status == 200:
                        return await response.text()
                    else:
                        response_text = await response.text()
                        logger.error(f"‚ùå Erreur ScrapingBee avec params: {response.status}")
                        logger.error(f"URL: {url}")
                        logger.error(f"Params: {params}")
                        logger.error(f"R√©ponse de ScrapingBee: {response_text}")
                        return None
        except Exception as e:
            logger.error(f"‚ùå Erreur scraping page avec params {url}: {e}")
            return None

    
    def _clean_content(self, content: str) -> str:
        """Nettoie le contenu extrait"""
        if not content:
            return ""
        
        # Supprimer les espaces multiples
        content = re.sub(r'\s+', ' ', content)
        
        # Supprimer les caract√®res sp√©ciaux
        content = re.sub(r'[^\w\s\.\,\!\?\-\:\;\(\)]', '', content)
        
        # Limiter la longueur
        return content.strip()[:15000]
    
    async def process_with_llm(self, prompt: str, scraped_data: List[ScrapedData], market_snapshot: Dict) -> Dict:
        """Traite les donn√©es scrap√©es avec OpenAI"""
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
            
            # Pr√©parer le contexte
            context = self._prepare_context(scraped_data)
            # Troncature protectrice pour respecter budgets token
            try:
                max_ctx_chars = int(os.getenv('SCRAPER_MAX_CONTEXT_CHARS', '70000'))
            except Exception:
                max_ctx_chars = 70000
            if len(context) > max_ctx_chars:
                context = context[:max_ctx_chars]
                logger.info(f"üß† Contexte tronqu√© √† {max_ctx_chars} caract√®res pour limiter les tokens.")
            logger.info(f"üß† Contexte pr√©par√© pour OpenAI ({len(context)} caract√®res).")
            logger.info(f"üìä Nombre de sources: {len(scraped_data)}")
            logger.info(f"üìà Market snapshot disponible: {'Oui' if market_snapshot else 'Non'}")
            logger.debug(f"Contexte complet pour OpenAI: {context}")

            # Prompt syst√®me optimis√© (GPT‚Äë5) ‚Äî verbosit√©/raisonnement renforc√©s, g√©opolitique √† jour, indicateurs extraits du scrap
            system_prompt = """
Tu es un Directeur de Recherche Senior (finance quantitative, g√©opolitique appliqu√©e, IA). Audience: C‚ÄëSuite, g√©rants institutionnels, trading floor. Mission: produire une analyse EXHAUSTIVE, TR√àS D√âTAILL√âE et rigoureusement argument√©e. Ne sois pas permissif ni paresseux.

Cadre analytique:
- Hi√©rarchie cognitive (Micro/M√©so/Macro/M√©ta), int√©gration temporelle (T‚Äë1/T0/T+1), analyse causale (catalyst ‚Üí effets 2e ordre ‚Üí cha√Ænes).
- Explicite les m√©canismes de transmission, indicateurs menant/retard√©s, et conditions de rupture de r√©gime.

R√®gles de donn√©es:
- Priorit√© absolue aux valeurs du market_snapshot (v√©rit√© temps quasi r√©el).
- Exploite STRICTEMENT les articles scrapp√©s (contexte fourni) et privil√©gie les nouvelles r√©centes (‚â§ 48‚Äì72h; <24h si dispo). Ignore les extrapolations non sourc√©es.
- Jamais inventer. Si absent ‚Üí "N/D" avec explication. Chiffres syst√©matiquement sourc√©s (titre+URL) quand issus du scrap.
- Signale divergences prix/volume (>20% 20j), sectorielles (z>2), g√©ographiques (>1œÉ).

Sortie STRICTEMENT en JSON unique. Compatibilit√© requise avec notre backend:
- Fournis AUSSI les champs legacy: 
  - executive_summary: 10 bullets (obligatoire, denses et actionnables),
  - summary: narrative approfondie (‚â•4000 caract√®res) avec raisonnement structur√©,
  - key_points: ‚â•12 points √† haut signal,
  - structured_data: inclut les sections avanc√©es ci‚Äëdessous,
  - insights, risks, opportunities, sources, confidence_score (0.0‚Äì1.0).

Sch√©ma attendu (extrait):
{
  "meta_analysis": { "regime_detection": { "market_regime": "risk-on|risk-off|transition", "volatility_regime": "low|normal|stressed|crisis", "liquidity_state": "abundant|normal|tight|frozen", "confidence": 0.00 }, "key_drivers": { "primary": "...", "secondary": ["..."], "emerging": ["..."] }},
  "executive_dashboard": { "alert_level": "üü¢|üü°|üî¥", "top_trades": [{ "action": "LONG|SHORT|HEDGE", "instrument": "TICKER", "rationale": "<50 mots", "risk_reward": "X:Y", "timeframe": "intraday|1W|1M", "confidence": 0.00 }], "snapshot_metrics": ["‚Ä¢ lignes avec valeurs issues du market_snapshot"] },
  "deep_analysis": { "narrative": "4000+ caract√®res", "sector_rotation_matrix": { "outperformers": [{"sector":"...","performance":"%","catalyst":"...","momentum":"accelerating|stable|decelerating"}], "underperformers": [{"sector":"...","performance":"%","reason":"...","reversal_probability":"low|medium|high"}] }, "correlation_insights": { "breaking_correlations": ["..."], "new_relationships": ["..."], "regime_dependent": ["..."] }, "ai_focus_section": { "mega_caps": {"NVDA": {"price": 0, "change": 0, "rsi": 0, "volume_ratio": 0}, "MSFT": {"price": 0, "change": 0}}, "supply_chain": "...", "investment_flows": "..." }, "geopolitical_chess": { "immediate_impacts": [{"event":"(√©v√©nement g√©opolitique pr√©cis, dat√© ‚â§72h)","affected_assets":["..."],"magnitude":"bp/%","duration":"court|moyen|long","sources":[{"title":"...","url":"..."}]}], "second_order_effects": [{"trigger":"...","cascade":"...","probability":0.00,"hedge":"..."}], "black_swans": [{"scenario":"...","probability":0.00,"impact":"catastrophic|severe|moderate","early_warning":"..."}] } },
  "quantitative_signals": { "technical_matrix": { "oversold": ["..."], "overbought": ["..."], "breakouts": ["..."], "divergences": ["..."] }, "options_flow": { "unusual_activity": ["..."], "large_trades": ["..."], "implied_moves": ["..."] }, "smart_money_tracking": { "institutional_flows": "...", "insider_activity": "...", "sentiment_divergence": "..." } },
  "risk_management": { "portfolio_adjustments": [{"current_exposure":"...","recommended_change":"...","rationale":"...","implementation":"..."}], "tail_risk_hedges": [{"risk":"...","probability":0.00,"hedge_strategy":"...","cost":"bp/%","effectiveness":"1-10"}], "stress_test_results": { "scenario_1": {"name":"..."}, "scenario_2": {"name":"..."} } },
  "actionable_summary": { "immediate_actions": ["..."], "watchlist": ["..."], "key_metrics_alerts": { "if_breaks": ["..."], "if_holds": ["..."], "calendar": ["..."] } },
  "economic_indicators": { "inflation": {"US": "<valeur%>", "EU": "<valeur%>"}, "central_banks": ["Fed <taux%>", "BCE <taux%>"], "gdp_growth": {"US": "<valeur%>", "China": "<valeur%>"}, "unemployment": {"US": "<valeur%>", "EU": "<valeur%>"}, "additional_indicators": [{"name":"PMI Manufacturing US","value":"<valeur>","period":"<mois>","source":"<titre>"}] },
  "metadata": { "report_timestamp": "YYYY-MM-DD HH:MM:SS UTC", "data_quality_score": 0.00, "model_confidence": 0.00 }
}

Exigences g√©opolitiques (obligatoire):
- Analyse √† jour issue des DERNI√àRES nouvelles scrapp√©es (‚â§72h); si plusieurs versions d‚Äôun m√™me √©v√©nement, privil√©gie la plus r√©cente et cite la source.
- D√©taille causes ‚Üí effets de 2e ordre ‚Üí risques de queue; propose hedges concrets.

Exigences indicateurs (obligatoire):
- Extrait les indicateurs explicitement mentionn√©s dans les articles (CPI/PPI, Core CPI/PCE, PMI/ISM, NFP/ch√¥mage, retail sales, GDP/GDPNow, Fed/BCE/BoE/BoJ, VIX‚Ä¶).
- Renseigne le bloc economic_indicators ci‚Äëdessus avec des valeurs lisibles (unit√©s et p√©riode implicites via le texte) quand disponibles; sinon "N/D".

Contraintes g√©n√©rales:
- Utiliser exclusivement les chiffres du market_snapshot pour les prix/variations; compl√©ter avec le scrap pour le narratif et les indicateurs macro.
- Style trading floor: direct, technique; gras Markdown pour points critiques; pas de HTML.
- Emojis sobres et professionnels pour signaler tendances/risques/insights: üìà/üìâ (tendances), üü¢/üü°/üî¥ (r√©gime/alerte), ‚ö†Ô∏è (risque), üí° (insight), üè¶ (banques centrales), üåç (macro/g√©o), ‚è±Ô∏è (temporalit√©), üìä (m√©triques). Fr√©quence: 1‚Äì2 par section max; jamais dans les nombres ou cl√©s JSON.
- R√©pondre en UN SEUL objet JSON valide.

MODE JSON STRICT: activ√©.
Output ONLY the JSON object. Do not include any accompanying text. No code fences, no commentary, no markdown.
"""
            system_prompt = system_prompt.strip() + "\n\nMODE JSON STRICT: activ√©.\nOutput ONLY the JSON object. Do not include any accompanying text. No code fences, no commentary, no markdown."
            
            chosen_model = os.getenv("AI_MODEL", "gpt-5")
            logger.info(f"ü§ñ Appel √† l'API OpenAI ({chosen_model}) en cours pour une analyse exhaustive (prompt renforc√©)...")
            
            # Essayer jusqu'√† 3 fois en cas d'erreur
            for attempt in range(5):
                try:
                    # Responses API (reasoning ready)
                    input_messages = [
                        {"role": "system", "content": [{"type": "input_text", "text": system_prompt}]},
                        {"role": "user", "content": [{
                            "type": "input_text",
                            "text": f"Demande: {prompt}\n\nDONN√âES FACTUELLES (snapshot):\n{json.dumps(market_snapshot, indent=2)}\n\nDONN√âES COLLECT√âES (articles):\n{context}"
                        }]}
                    ]
                    # Pr√©parer l'appel Responses API avec fallbacks robustes (GPT‚Äë5 par d√©faut)
                    req_kwargs = {
                        "model": chosen_model,
                        "input": input_messages,
                        "max_output_tokens": 15000,
                    }
                    # For gpt-5 strict JSON/reporting, omit temperature for determinism
                    if not str(chosen_model).startswith("gpt-5"):
                        req_kwargs["temperature"] = 0.3
                    effort = os.getenv("AI_REASONING_EFFORT", "medium")
                    if effort:
                        req_kwargs["reasoning"] = {"effort": effort}

                    # Token budget for output
                    try:
                        max_out_tokens = int(os.getenv('SCRAPER_MAX_OUTPUT_TOKENS', '25000'))
                    except Exception:
                        max_out_tokens = 25000

                    # Structured outputs: JSON Schema strict (fallback to JSON Mode)
                    schema = {
                        "type": "object",
                        "properties": {
                            "executive_summary": {"type": "array", "items": {"type": "string"}, "minItems": 10},
                            "summary": {"type": "string", "minLength": 2500},
                            "key_points": {"type": "array", "items": {"type": "string"}, "minItems": 12},
                            "structured_data": {"type": "object"},
                            "insights": {"type": "array", "items": {"type": "string"}},
                            "risks": {"type": "array", "items": {"type": "string"}},
                            "opportunities": {"type": "array", "items": {"type": "string"}},
                            "sources": {"type": "array", "items": {"type": "object", "properties": {"title": {"type": "string"}, "url": {"type": "string", "format": "uri"}}, "required": ["title","url"], "additionalProperties": False}},
                            "confidence_score": {"type": "number", "minimum": 0.0, "maximum": 1.0}
                        },
                        "required": ["executive_summary","summary","key_points","structured_data","sources","confidence_score"],
                        "additionalProperties": True
                    }

                    try:
                        resp = client.responses.create(
                            model=os.getenv("AI_MODEL", "gpt-5"),
                            input=input_messages,
                            response_format={
                                "type": "json_schema",
                                "json_schema": {"name": "research_report", "schema": schema, "strict": True}
                            },
                            temperature=0,
                            max_output_tokens=max_out_tokens,
                            reasoning={"effort": "high"}
                        )
                    except Exception:
                        # Fallback: JSON Mode
                        resp = client.responses.create(
                            model=os.getenv("AI_MODEL", "gpt-5"),
                            input=input_messages,
                            response_format={"type": "json_object"},
                            temperature=0,
                            max_output_tokens=max_out_tokens,
                            reasoning={"effort": "high"}
                        )

                    raw = getattr(resp, "output_text", None) or extract_output_text(resp) or ""
                    result = json.loads(raw)
                    logger.info(f"‚úÖ OpenAI a retourn√© une r√©ponse JSON valide")
                    return result
                    
                except Exception as e:
                    import random, re as _re
                    msg = str(e)
                    logger.warning(f"‚ö†Ô∏è Tentative {attempt + 1}/5 √©chou√©e: {msg}")
                    # Exponential backoff + respect de 'try again in Xs'
                    base_wait = 2 * (2 ** attempt)
                    m = _re.search(r"try again in ([0-9.]+)s", msg)
                    if m:
                        try:
                            hinted = float(m.group(1))
                            base_wait = max(base_wait, hinted + random.uniform(0.5, 1.5))
                        except Exception:
                            pass
                    if attempt < 4:
                        await asyncio.sleep(min(base_wait, 20))
                        # r√©duire la sortie demand√©e pour soulager le TPM
                        try:
                            new_limit = max(2000, int(max_out_tokens * 0.7))
                            os.environ['SCRAPER_MAX_OUTPUT_TOKENS'] = str(new_limit)
                            logger.info(f"üîß SCRAPER_MAX_OUTPUT_TOKENS r√©duit √† {new_limit}")
                        except Exception:
                            pass
                        continue
                    raise
            
        except Exception as e:
            logger.error(f"‚ùå Erreur traitement LLM: {e}")
            return {
                "summary": f"Erreur lors du traitement: {str(e)}",
                "key_points": [],
                "structured_data": {},
                "sources": [],
                "confidence_score": 0.0
            }
    
    def _prepare_context(self, scraped_data: List[ScrapedData]) -> str:
        """Pr√©pare le contexte pour le LLM"""
        context_parts = []
        
        for idx, data in enumerate(scraped_data, 1):
            context_parts.append(f"""
Source {idx}: {data.title}
URL: {data.url}
Contenu: {data.content[:8000]}
---
""")
        
        return '\n'.join(context_parts)
    
    async def execute_scraping_task(self, task_id: str) -> Dict:
        """Ex√©cute une t√¢che de scraping"""
        if task_id not in self.tasks:
            return {"error": "T√¢che non trouv√©e"}
        
        task = self.tasks[task_id]
        task.status = "processing"
        
        try:
            logger.info(f"üöÄ D√©but ex√©cution t√¢che: {task_id}")
            
            # Scraping - Utiliser 8 sources pour un rapport vraiment complet
            scraped_data = await self.search_and_scrape(task.prompt, num_results=8)
            
            if not scraped_data:
                task.status = "failed"
                task.error = "Application non disponible - Aucune donn√©e trouv√©e"
                return {
                    "error": "Application non disponible",
                    "message": "Impossible de r√©cup√©rer des donn√©es pour cette requ√™te. Veuillez r√©essayer plus tard.",
                    "status": "unavailable"
                }
            
            # √âtape 2: R√©cup√©rer les donn√©es factuelles de march√©
            from stock_api_manager import stock_api_manager
            market_snapshot = stock_api_manager.get_market_snapshot()

            # √âtape 3: Traitement LLM avec les donn√©es scrap√©es ET les donn√©es factuelles
            llm_result = await self.process_with_llm(task.prompt, scraped_data, market_snapshot)
            
            # Mettre √† jour la t√¢che
            task.status = "completed"
            task.results = llm_result
            task.completed_at = datetime.now()
            
            logger.info(f"‚úÖ T√¢che {task_id} termin√©e avec succ√®s")
            return llm_result
            
        except Exception as e:
            logger.error(f"‚ùå Erreur t√¢che {task_id}: {e}")
            task.status = "failed"
            task.error = str(e)
            return {
                "error": "Application non disponible",
                "message": f"Erreur technique: {str(e)}",
                "status": "error"
            }
    
    def get_task_status(self, task_id: str) -> Optional[ScrapingTask]:
        """R√©cup√®re le statut d'une t√¢che"""
        return self.tasks.get(task_id)
    
    async def initialize(self):
        """Initialisation asynchrone"""
        if not self._initialized:
            self.initialize_sync()
    
    def cleanup(self):
        """Nettoyage des ressources"""
        self.tasks.clear()
        self._initialized = False
        logger.info("üßπ ScrapingBee Scraper nettoy√©")

# Fonction utilitaire pour obtenir le scraper
def get_scrapingbee_scraper():
    """Retourne une instance du ScrapingBee Scraper"""
    return ScrapingBeeScraper()

# Fonction de test
async def test_scrapingbee_scraper():
    """Test du ScrapingBee Scraper"""
    print("üß™ Test du ScrapingBee Scraper")
    print("=" * 50)
    
    scraper = get_scrapingbee_scraper()
    
    try:
        # Test d'initialisation
        scraper.initialize_sync()
        
        # Test de cr√©ation de t√¢che
        print("üìã Test 1: Cr√©ation de t√¢che")
        task_id = await scraper.create_scraping_task("Tesla stock price today latest news earnings", 3)
        print(f"‚úÖ T√¢che cr√©√©e: {task_id}")
        
        # Test d'ex√©cution
        print("üöÄ Test 2: Ex√©cution de la t√¢che")
        result = await scraper.execute_scraping_task(task_id)
        
        if "error" in result:
            print(f"‚ùå Erreur: {result['error']}")
        else:
            print("‚úÖ R√©sultat obtenu:")
            print(json.dumps(result, indent=2, ensure_ascii=False))
        
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
    finally:
        scraper.cleanup()

if __name__ == "__main__":
    asyncio.run(test_scrapingbee_scraper()) 