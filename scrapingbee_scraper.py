import os
import asyncio
import aiohttp
import json
import logging
from datetime import datetime
from typing import List, Dict, Optional
from dataclasses import dataclass
from urllib.parse import quote_plus
import re

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Charger les variables d'environnement de maniÃ¨re sÃ©curisÃ©e
try:
    from dotenv import load_dotenv
    load_dotenv()
except Exception as e:
    logger.warning(f"âš ï¸ Impossible de charger .env: {e}")

@dataclass
class ScrapedData:
    url: str
    title: str
    content: str
    timestamp: datetime
    metadata: Dict

@dataclass
class ScrapingTask:
    id: str
    prompt: str
    status: str  # pending, processing, completed, failed
    results: Optional[Dict] = None
    created_at: Optional[datetime] = None
    completed_at: Optional[datetime] = None
    error: Optional[str] = None

class ScrapingBeeScraper:
    def __init__(self):
        self.api_key = os.getenv('SCRAPINGBEE_API_KEY')
        self.base_url = "https://app.scrapingbee.com/api/v1"
        self.tasks = {}
        self._initialized = False
        
        if not self.api_key:
            logger.warning("âš ï¸ SCRAPINGBEE_API_KEY non configurÃ©")
        
    def initialize_sync(self):
        """Initialisation synchrone du scraper"""
        try:
            if not self._initialized:
                if not self.api_key:
                    raise Exception("SCRAPINGBEE_API_KEY requis")
                
                self._initialized = True
                logger.info("âœ… ScrapingBee Scraper initialisÃ©")
        except Exception as e:
            logger.error(f"âŒ Erreur initialisation ScrapingBee: {e}")
            raise
    
    async def create_scraping_task(self, prompt: str, num_results: int = 5) -> str:
        """CrÃ©e une nouvelle tÃ¢che de scraping"""
        import uuid
        task_id = str(uuid.uuid4())
        
        task = ScrapingTask(
            id=task_id,
            prompt=prompt,
            status="pending",
            created_at=datetime.now()
        )
        
        self.tasks[task_id] = task
        logger.info(f"ğŸ“‹ TÃ¢che de scraping crÃ©Ã©e: {task_id}")
        return task_id
    
    async def search_and_scrape(self, query: str, num_results: int = 5) -> List[ScrapedData]:
        """Recherche sur Google et scrape les rÃ©sultats avec ScrapingBee"""
        if not self._initialized:
            await self.initialize()
        
        results = []
        
        try:
            logger.info(f"ğŸ” Recherche ScrapingBee: {query}")
            
            # Ã‰tape 1: Recherche Google avec ScrapingBee
            search_results = await self._search_google(query, num_results)
            
            if not search_results:
                logger.warning("âš ï¸ Aucun rÃ©sultat de recherche trouvÃ©")
                return results
            
            # Ã‰tape 2: Scraping des pages avec ScrapingBee
            for i, result in enumerate(search_results[:num_results]):
                try:
                    logger.info(f"ğŸ“– Scraping {i+1}/{len(search_results)}: {result['title']}")
                    
                    scraped_content = await self._scrape_page(result['url'])
                    
                    if scraped_content:
                        results.append(ScrapedData(
                            url=result['url'],
                            title=result['title'],
                            content=scraped_content,
                            timestamp=datetime.now(),
                            metadata={
                                'word_count': len(scraped_content.split()),
                                'language': 'fr',
                                'source': 'scrapingbee'
                            }
                        ))
                    
                except Exception as e:
                    logger.error(f"âŒ Erreur scraping {result['url']}: {e}")
                    # Ajouter un rÃ©sultat d'erreur
                    results.append(ScrapedData(
                        url=result['url'],
                        title=result['title'],
                        content=f"Erreur lors du scraping: {str(e)}",
                        timestamp=datetime.now(),
                        metadata={
                            'word_count': 10,
                            'language': 'fr',
                            'error': str(e)
                        }
                    ))
            
        except Exception as e:
            logger.error(f"âŒ Erreur recherche/scraping: {e}")
        
        return results
    
    async def _search_google(self, query: str, num_results: int = 5) -> List[Dict]:
        """Recherche sur des sites financiers directs avec ScrapingBee"""
        try:
            # DÃ©tecter si c'est une requÃªte sur les marchÃ©s gÃ©nÃ©raux ou une action spÃ©cifique
            query_lower = query.lower()
            
            if any(word in query_lower for word in ['marchÃ©', 'marchÃ©s', 'financier', 'financiers', 'situation', 'aujourd\'hui', 'ia', 'ai', 'intelligence']):
                # Sites d'actualitÃ©s financiÃ¨res pour les requÃªtes gÃ©nÃ©rales
                sites_financiers = [
                    {
                        'url': "https://www.reuters.com/markets/",
                        'title': "Reuters - MarchÃ©s financiers"
                    },
                    {
                        'url': "https://www.bloomberg.com/markets",
                        'title': "Bloomberg - MarchÃ©s"
                    },
                    {
                        'url': "https://www.ft.com/markets",
                        'title': "Financial Times - MarchÃ©s"
                    },
                    {
                        'url': "https://www.cnbc.com/markets/",
                        'title': "CNBC - MarchÃ©s"
                    },
                    {
                        'url': "https://www.marketwatch.com/",
                        'title': "MarketWatch - ActualitÃ©s"
                    }
                ]
            else:
                # Sites de stocks pour les requÃªtes spÃ©cifiques
                stock_symbol = query.split()[0].upper()
                sites_financiers = [
                    {
                        'url': f"https://finance.yahoo.com/quote/{stock_symbol}",
                        'title': f"Yahoo Finance - {stock_symbol}"
                    },
                    {
                        'url': f"https://www.marketwatch.com/investing/stock/{stock_symbol.lower()}",
                        'title': f"MarketWatch - {stock_symbol}"
                    },
                    {
                        'url': f"https://www.reuters.com/companies/{stock_symbol}.O",
                        'title': f"Reuters - {stock_symbol}"
                    },
                    {
                        'url': f"https://www.bloomberg.com/quote/{stock_symbol}:US",
                        'title': f"Bloomberg - {stock_symbol}"
                    }
                ]
            
            logger.info(f"ğŸ” Recherche sur {min(len(sites_financiers), num_results)} sites financiers")
            
            # Retourner les sites financiers comme rÃ©sultats de recherche
            return sites_financiers[:num_results]
                        
        except Exception as e:
            logger.error(f"âŒ Erreur recherche sites financiers: {e}")
            return []  # Pas de fallback
    
    def _extract_links_from_html(self, html_content: str, query: str) -> List[Dict]:
        """Extrait les liens depuis le HTML de Google"""
        import re
        
        links = []
        
        # Patterns amÃ©liorÃ©s pour extraire les liens des rÃ©sultats Google
        patterns = [
            # Pattern pour les rÃ©sultats de recherche Google standard
            r'<a[^>]*href="([^"]*)"[^>]*>([^<]*)</a>',
            # Pattern pour les titres de rÃ©sultats
            r'<h3[^>]*><a[^>]*href="([^"]*)"[^>]*>([^<]*)</a></h3>',
            # Pattern pour les divs avec titres
            r'<div[^>]*class="[^"]*title[^"]*"[^>]*><a[^>]*href="([^"]*)"[^>]*>([^<]*)</a></div>',
            # Pattern pour les liens dans les rÃ©sultats de recherche
            r'<a[^>]*href="([^"]*)"[^>]*class="[^"]*"[^>]*>([^<]*)</a>',
            # Pattern gÃ©nÃ©rique pour tous les liens
            r'<a[^>]*href="([^"]*)"[^>]*>([^<]*)</a>'
        ]
        
        logger.info(f"ğŸ” Extraction de liens depuis {len(html_content)} caractÃ¨res")
        
        for i, pattern in enumerate(patterns):
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            logger.info(f"ğŸ” Pattern {i+1}: {len(matches)} matches trouvÃ©s")
            
            for url, title in matches:
                logger.debug(f"ğŸ” Match trouvÃ©: URL='{url[:100]}...' Title='{title[:100]}...'")
                
                # Nettoyer l'URL (enlever les paramÃ¨tres Google)
                original_url = url
                if url.startswith('/url?q='):
                    url = url.split('/url?q=')[1].split('&')[0]
                elif url.startswith('/url?'):
                    url = url.split('/url?')[1].split('&')[0]
                
                logger.debug(f"ğŸ” URL nettoyÃ©e: {url[:100]}...")
                
                # VÃ©rifier si c'est un lien valide
                is_valid = True
                reason = ""
                
                if not url.startswith('http'):
                    is_valid = False
                    reason = "Pas HTTP"
                elif url.startswith('https://www.google.com'):
                    is_valid = False
                    reason = "Google.com"
                elif url.startswith('https://accounts.google.com'):
                    is_valid = False
                    reason = "Accounts Google"
                elif url.startswith('https://maps.google.com'):
                    is_valid = False
                    reason = "Maps Google"
                elif url.startswith('https://support.google.com'):
                    is_valid = False
                    reason = "Support Google"
                elif len(title.strip()) <= 5:
                    is_valid = False
                    reason = "Titre trop court"
                elif any(domain in url.lower() for domain in ['google.com', 'youtube.com', 'facebook.com', 'instagram.com']):
                    is_valid = False
                    reason = "Domaine exclu"
                
                if is_valid:
                    # Nettoyer le titre
                    title = re.sub(r'<[^>]+>', '', title).strip()
                    
                    if title and len(title) > 5:
                        links.append({
                            'url': url,
                            'title': title
                        })
                        logger.info(f"âœ… Lien trouvÃ©: {title[:50]}... -> {url}")
                    else:
                        logger.debug(f"âŒ Titre invalide aprÃ¨s nettoyage: '{title}'")
                else:
                    logger.debug(f"âŒ Lien rejetÃ© ({reason}): {url[:50]}...")
        
        # Supprimer les doublons
        seen_urls = set()
        unique_links = []
        for link in links:
            if link['url'] not in seen_urls:
                seen_urls.add(link['url'])
                unique_links.append(link)
        
        logger.info(f"ğŸ“„ Total: {len(unique_links)} liens uniques extraits")
        return unique_links
    

    
    async def _scrape_page(self, url: str) -> Optional[str]:
        """Scrape une page avec ScrapingBee"""
        try:
            # ParamÃ¨tres ScrapingBee pour le scraping
            params = {
                'api_key': self.api_key,
                'url': url,
                'render_js': 'true',  # Rendu JS pour les pages dynamiques
                'premium_proxy': 'false',  # Proxy standard pour Ã©conomiser les crÃ©dits
                'country_code': 'us'
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.get(self.base_url, params=params) as response:
                    if response.status == 200:
                        # ScrapingBee retourne du HTML, pas du JSON
                        html_content = await response.text()
                        
                        # Extraire le contenu du body
                        cleaned_content = self._extract_text_from_html(html_content)
                        logger.info(f"ğŸ“„ Contenu extrait de {url}: {len(cleaned_content)} caractÃ¨res.")
                        logger.debug(f"Contenu brut (aperÃ§u): {cleaned_content[:500]}...")

                        return cleaned_content[:8000]  # Limite de caractÃ¨res augmentÃ©e
                    else:
                        logger.error(f"âŒ Erreur ScrapingBee scraping: {response.status}")
                        return None
                        
        except Exception as e:
            logger.error(f"âŒ Erreur scraping page {url}: {e}")
            return None
    
    def _extract_text_from_html(self, html_content: str) -> str:
        """Extrait le texte du HTML"""
        if not html_content:
            return ""
        
        # Supprimer les balises script et style
        html_content = re.sub(r'<script[^>]*>.*?</script>', '', html_content, flags=re.DOTALL)
        html_content = re.sub(r'<style[^>]*>.*?</style>', '', html_content, flags=re.DOTALL)
        
        # Extraire le texte des balises
        text = re.sub(r'<[^>]+>', ' ', html_content)
        
        # Nettoyer le texte
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\w\s\.\,\!\?\-\:\;\(\)\-\$\%]', '', text)
        
        return text.strip()[:8000]
    
    def _clean_content(self, content: str) -> str:
        """Nettoie le contenu extrait"""
        if not content:
            return ""
        
        # Supprimer les espaces multiples
        content = re.sub(r'\s+', ' ', content)
        
        # Supprimer les caractÃ¨res spÃ©ciaux
        content = re.sub(r'[^\w\s\.\,\!\?\-\:\;\(\)]', '', content)
        
        # Limiter la longueur
        return content.strip()[:8000]
    
    async def process_with_llm(self, prompt: str, scraped_data: List[ScrapedData], market_snapshot: Dict) -> Dict:
        """Traite les donnÃ©es scrapÃ©es avec OpenAI"""
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
            
            # PrÃ©parer le contexte
            context = self._prepare_context(scraped_data)
            logger.info(f"ğŸ§  Contexte prÃ©parÃ© pour OpenAI ({len(context)} caractÃ¨res).")
            logger.debug(f"Contexte complet pour OpenAI: {context}")

            # Prompt systÃ¨me enrichi
            system_prompt = """Tu es un expert analyste financier de classe mondiale. Ta mission est de produire un rapport de marchÃ© EXHAUSTIF, DÃ‰TAILLÃ‰ et HAUTEMENT STRUCTURÃ‰. Combine les donnÃ©es factuelles (market_snapshot) avec l'analyse des textes (donnÃ©es collectÃ©es).

STRUCTURE OBLIGATOIRE DE LA RÃ‰PONSE JSON :
{
    "market_snapshot": {
        "indices": {
            "S&P 500": {"price": 5447.87, "change": -8.55, "change_percent": -0.16},
            "NASDAQ": {"price": 17689.36, "change": -28.20, "change_percent": -0.16}
        },
        "commodities": {"Gold": {"price": 2330.20, "change": -1.20, "change_percent": -0.05}},
        "crypto": {"Bitcoin": {"price": 69304.58, "change": 450.15, "change_percent": 0.65}}
    },
    "summary": "Un rÃ©sumÃ© exÃ©cutif substantiel et approfondi. IntÃ¨gre les donnÃ©es du snapshot pour contextualiser l'analyse. Minimum 500 mots.",
    "key_points": [
        "Point clÃ© dÃ©taillÃ© 1, intÃ©grant une donnÃ©e factuelle si pertinent", 
        "...",
        "Point clÃ© dÃ©taillÃ© 10"
    ],
    "structured_data": {
        "market_sentiment": "Analyse du sentiment de marchÃ© (haussier, baissier, neutre) avec justification.",
        "key_trends": ["Tendance majeure 1 identifiÃ©e", "Tendance majeure 2", "..."],
        "major_events": ["Ã‰vÃ©nement majeur 1 et son impact", "Ã‰vÃ©nement majeur 2", "..."],
        "sector_analysis": "Analyse dÃ©taillÃ©e des secteurs mentionnÃ©s, en particulier l'IA."
    },
    "insights": ["Insight actionnable 1 basÃ© sur une corrÃ©lation de donnÃ©es", "..."],
    "risks": ["Risque potentiel 1 avec explication", "..."],
    "opportunities": ["OpportunitÃ© d'investissement 1 avec justification", "..."],
    "sources_analysis": "Une brÃ¨ve critique de la fiabilitÃ© des sources textuelles fournies.",
    "confidence_score": 0.95,
    "sources": [{"title": "Titre de la source 1", "url": "URL de la source 1"}]
}"""
            
            logger.info("ğŸ¤– Appel Ã  l'API OpenAI (gpt-4o) en cours pour une analyse exhaustive...")
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"Demande: {prompt}\n\nDONNÃ‰ES FACTUELLES (snapshot):\n{json.dumps(market_snapshot, indent=2)}\n\nDONNÃ‰ES COLLECTÃ‰ES (articles):\n{context}"}
                ],
                response_format={"type": "json_object"},
                temperature=0.2,
                max_tokens=4000
            )
            
            return json.loads(response.choices[0].message.content)
            
        except Exception as e:
            logger.error(f"âŒ Erreur traitement LLM: {e}")
            return {
                "summary": f"Erreur lors du traitement: {str(e)}",
                "key_points": [],
                "structured_data": {},
                "sources": [],
                "confidence_score": 0.0
            }
    
    def _prepare_context(self, scraped_data: List[ScrapedData]) -> str:
        """PrÃ©pare le contexte pour le LLM"""
        context_parts = []
        
        for idx, data in enumerate(scraped_data, 1):
            context_parts.append(f"""
Source {idx}: {data.title}
URL: {data.url}
Contenu: {data.content[:4000]}
---
""")
        
        return '\n'.join(context_parts)
    
    async def execute_scraping_task(self, task_id: str) -> Dict:
        """ExÃ©cute une tÃ¢che de scraping"""
        if task_id not in self.tasks:
            return {"error": "TÃ¢che non trouvÃ©e"}
        
        task = self.tasks[task_id]
        task.status = "processing"
        
        try:
            logger.info(f"ğŸš€ DÃ©but exÃ©cution tÃ¢che: {task_id}")
            
            # Scraping - Utiliser 3 sources pour un rapport plus complet
            scraped_data = await self.search_and_scrape(task.prompt, num_results=3)
            
            if not scraped_data:
                task.status = "failed"
                task.error = "Application non disponible - Aucune donnÃ©e trouvÃ©e"
                return {
                    "error": "Application non disponible",
                    "message": "Impossible de rÃ©cupÃ©rer des donnÃ©es pour cette requÃªte. Veuillez rÃ©essayer plus tard.",
                    "status": "unavailable"
                }
            
            # Ã‰tape 2: RÃ©cupÃ©rer les donnÃ©es factuelles de marchÃ©
            from stock_api_manager import stock_api_manager
            market_snapshot = stock_api_manager.get_market_snapshot()

            # Ã‰tape 3: Traitement LLM avec les donnÃ©es scrapÃ©es ET les donnÃ©es factuelles
            llm_result = await self.process_with_llm(task.prompt, scraped_data, market_snapshot)
            
            # Mettre Ã  jour la tÃ¢che
            task.status = "completed"
            task.results = llm_result
            task.completed_at = datetime.now()
            
            logger.info(f"âœ… TÃ¢che {task_id} terminÃ©e avec succÃ¨s")
            return llm_result
            
        except Exception as e:
            logger.error(f"âŒ Erreur tÃ¢che {task_id}: {e}")
            task.status = "failed"
            task.error = str(e)
            return {
                "error": "Application non disponible",
                "message": f"Erreur technique: {str(e)}",
                "status": "error"
            }
    
    def get_task_status(self, task_id: str) -> Optional[ScrapingTask]:
        """RÃ©cupÃ¨re le statut d'une tÃ¢che"""
        return self.tasks.get(task_id)
    
    async def initialize(self):
        """Initialisation asynchrone"""
        if not self._initialized:
            self.initialize_sync()
    
    def cleanup(self):
        """Nettoyage des ressources"""
        self.tasks.clear()
        self._initialized = False
        logger.info("ğŸ§¹ ScrapingBee Scraper nettoyÃ©")

# Fonction utilitaire pour obtenir le scraper
def get_scrapingbee_scraper():
    """Retourne une instance du ScrapingBee Scraper"""
    return ScrapingBeeScraper()

# Fonction de test
async def test_scrapingbee_scraper():
    """Test du ScrapingBee Scraper"""
    print("ğŸ§ª Test du ScrapingBee Scraper")
    print("=" * 50)
    
    scraper = get_scrapingbee_scraper()
    
    try:
        # Test d'initialisation
        scraper.initialize_sync()
        
        # Test de crÃ©ation de tÃ¢che
        print("ğŸ“‹ Test 1: CrÃ©ation de tÃ¢che")
        task_id = await scraper.create_scraping_task("Tesla stock price today latest news earnings", 3)
        print(f"âœ… TÃ¢che crÃ©Ã©e: {task_id}")
        
        # Test d'exÃ©cution
        print("ğŸš€ Test 2: ExÃ©cution de la tÃ¢che")
        result = await scraper.execute_scraping_task(task_id)
        
        if "error" in result:
            print(f"âŒ Erreur: {result['error']}")
        else:
            print("âœ… RÃ©sultat obtenu:")
            print(json.dumps(result, indent=2, ensure_ascii=False))
        
    except Exception as e:
        print(f"âŒ Erreur: {e}")
    finally:
        scraper.cleanup()

if __name__ == "__main__":
    asyncio.run(test_scrapingbee_scraper()) 